document.addEventListener("DOMContentLoaded", function () {
    const searchInput = document.getElementById("search-bar");
    const searchButton = document.getElementById("search-btn");
    const scrapItemsContainer = document.querySelector(".dyn-scrap-items");
    const loadingSpinner = document.querySelector(".dyn-loading");
    const searchTitle = document.getElementById("dyn-search-title");

    function showLoading() {
        loadingSpinner.style.display = "block"; 
        scrapItemsContainer.style.display = "none"; 
        searchTitle.innerHTML = ''
    }

    function hideLoading() {
        loadingSpinner.style.display = "none"; 
        scrapItemsContainer.style.display = "grid";
    }

    async function fetchResearchPapers(query) {

        showLoading();
        
        try {
            /*const response = await fetch('/scrap', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ 'message': query }),
            });
    
            if (!response.ok) {
                throw new Error(`HTTP error! Status: ${response.status}`);
            }

            const msg = response;

            if (msg === "Error! Network Failure" || msg === 'Error! Unauthorized Access'){

                alert("Error in Scraping Research Papers");
                
                return;
            
            }

            const data = await response.json();*/
    
            searchTitle.textContent = `Search Results for : ${query}`;
            
            scrapItemsContainer.innerHTML = '';

            data = [{"title": "CF-CAM: Gradient Perturbation Mitigation and Feature Stabilization for Reliable Interpretability", "authors": ["Hongjie He"], "date": "April 2025", "doi": "arXiv:2504.00060", "url": "https://arxiv.org/abs/2504.00060", "pdf": "https://arxiv.org/pdf/2504.00060", "abstract": "As deep learning continues to advance, the opacity of neural network decision-making remains a critical challenge, limiting trust and applicability in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged as a key approach to visualizing model decisions, yet existing methods face inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to gradient perturbations, leading to unstable and unreliable explanations. Conversely, gradient-free approaches mitigate gradient instability but incur significant computational overhead and inference latency. To address these limitations, we propose Cluster Filter Class Activation Map (CF-CAM), a novel framework that reintroduces gradient-based weighting while enhancing robustness against gradient noise. CF-CAM employs a hierarchical importance weighting strategy to balance discriminative feature preservation and noise elimination. A density-aware channel clustering via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups semantically relevant feature channels and discard noise-prone activations. Additionally, cluster-conditioned gradient filtering leverages bilateral filters to refine gradient signals, preserving edge-aware localization while suppressing noise impact. Experiment results demonstrate that CF-CAM achieves superior interpretability performance while maintaining resilience to gradient perturbations, outperforming state-of-the-art CAM methods in faithfulness and robustness. By effectively mitigating gradient instability without excessive computational cost, CF-CAM provides a reliable solution for enhancing the interpretability of deep neural networks in critical applications such as medical diagnosis and autonomous driving.", "field": ["Machine Learning", "Artificial Intelligence", "Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDA2MA"}, {"title": "CyberBOT: Towards Reliable Cybersecurity Education via Ontology-Grounded Retrieval Augmented Generation", "authors": ["Chengshuai Zhao"], "date": "April 2025", "doi": "arXiv:2504.00389", "url": "https://arxiv.org/abs/2504.00389", "pdf": "https://arxiv.org/pdf/2504.00389", "abstract": "Advancements in large language models (LLMs) have enabled the development of intelligent educational tools that support inquiry-based learning across technical domains. In cybersecurity education, where accuracy and safety are paramount, systems must go beyond surface-level relevance to provide information that is both trustworthy and domain-appropriate. To address this challenge, we introduce CyberBOT, a question-answering chatbot that leverages a retrieval-augmented generation (RAG) pipeline to incorporate contextual information from course-specific materials and validate responses using a domain-specific cybersecurity ontology. The ontology serves as a structured reasoning layer that constrains and verifies LLM-generated answers, reducing the risk of misleading or unsafe guidance. CyberBOT has been deployed in a large graduate-level course at Arizona State University (ASU), where more than one hundred students actively engage with the system through a dedicated web-based platform. Computational evaluations in lab environments highlight the potential capacity of CyberBOT, and a forthcoming field study will evaluate its pedagogical impact. By integrating structured domain reasoning with modern generative capabilities, CyberBOT illustrates a promising direction for developing reliable and curriculum-aligned AI applications in specialized educational contexts.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDM4OQ"}, {"title": "BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks", "authors": ["Wei Li"], "date": "April 2025", "doi": "arXiv:2504.05180", "url": "https://arxiv.org/abs/2504.05180", "pdf": "https://arxiv.org/pdf/2504.05180", "abstract": "While many EDA tasks already involve graph-based data, existing LLMs in EDA primarily either represent graphs as sequential text, or simply ignore graph-structured data that might be beneficial like dataflow graphs of RTL code. Recent studies have found that LLM performance suffers when graphs are represented as sequential text, and using additional graph information significantly boosts performance. To address these challenges, we introduce BRIDGES, a framework designed to incorporate graph modality into LLMs for EDA tasks. BRIDGES integrates an automated data generation workflow, a solution that combines graph modality with LLM, and a comprehensive evaluation suite. First, we establish an LLM-driven workflow to generate RTL and netlist-level data, converting them into dataflow and netlist graphs with function descriptions. This workflow yields a large-scale dataset comprising over 500,000 graph instances and more than 1.5 billion tokens. Second, we propose a lightweight cross-modal projector that encodes graph representations into text-compatible prompts, enabling LLMs to effectively utilize graph data without architectural modifications. Experimental results demonstrate 2x to 10x improvements across multiple tasks compared to text-only baselines, including accuracy in design retrieval, type prediction and perplexity in function description, with negligible computational overhead (<1% model weights increase and <30% additional runtime overhead). Even without additional LLM finetuning, our results outperform text-only by a large margin. We plan to release BRIDGES, including the dataset, models, and training flow.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTE4MA"}, {"title": "Graph-based Diffusion Model for Collaborative Filtering", "authors": ["Xuan Zhang"], "date": "April 2025", "doi": "arXiv:2504.05029", "url": "https://arxiv.org/abs/2504.05029", "pdf": "https://arxiv.org/pdf/2504.05029", "abstract": "Recently, diffusion-based recommendation methods have achieved impressive results. However, existing approaches predominantly treat each user's historical interactions as independent training samples, overlooking the potential of higher-order collaborative signals between users and items. Such signals, which encapsulate richer and more nuanced relationships, can be naturally captured using graph-based data structures. To address this limitation, we extend diffusion-based recommendation methods to the graph domain by directly modeling user-item bipartite graphs with diffusion models. This enables better modeling of the higher-order connectivity inherent in complex interaction dynamics. However, this extension introduces two primary challenges: (1) Noise Heterogeneity, where interactions are influenced by various forms of continuous and discrete noise, and (2) Relation Explosion, referring to the high computational costs of processing large-scale graphs. To tackle these challenges, we propose a Graph-based Diffusion Model for Collaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a multi-level noise corruption mechanism that integrates both continuous and discrete noise, effectively simulating real-world interaction complexities. To mitigate relation explosion, we design a user-active guided diffusion process that selectively focuses on the most meaningful edges and active users, reducing inference costs while preserving the graph's topological integrity. Extensive experiments on three benchmark datasets demonstrate that GDMCF consistently outperforms state-of-the-art methods, highlighting its effectiveness in capturing higher-order collaborative signals and improving recommendation performance.", "field": ["Social and Information Networks", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTAyOQ"}, {"title": "Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent", "authors": ["Bo Chen"], "date": "April 2025", "doi": "arXiv:2504.04702", "url": "https://arxiv.org/abs/2504.04702", "pdf": "https://arxiv.org/pdf/2504.04702", "abstract": "Recent advancements in Transformer-based architectures have led to impressive breakthroughs in natural language processing tasks, with models such as GPT-4, Claude, and Gemini demonstrating human-level reasoning abilities. However, despite their high performance, concerns remain about the inherent limitations of these models, especially when it comes to learning basic logical functions. While complexity-theoretic analyses indicate that Transformers can represent simple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majority gates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these results assume ideal parameter settings and do not account for the constraints imposed by gradient descent-based training methods. In this work, we investigate whether Transformers can truly learn simple majority functions when trained using gradient-based methods. We focus on a simplified variant of the Transformer architecture and consider both $n=\\mathrm{poly}(d)$ and $n=\\exp(\u03a9(d))$ number of training samples, where each sample is a $d$-size binary string paired with the output of a basic majority function. Our analysis demonstrates that even after $\\mathrm{poly}(d)$ gradient queries, the generalization error of the Transformer model still remains substantially large, growing exponentially with $d$. This work highlights fundamental optimization challenges in training Transformers for the simplest logical reasoning tasks and provides new insights into their theoretical limitations.", "field": ["Machine Learning", "Artificial Intelligence", "Computational Complexity"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDcwMg"}, {"title": "Three-Factor Learning in Spiking Neural Networks: An Overview of Methods and Trends from a Machine Learning Perspective", "authors": ["Szymon Mazurek"], "date": "April 2025", "doi": "arXiv:2504.05341", "url": "https://arxiv.org/abs/2504.05341", "pdf": "https://arxiv.org/pdf/2504.05341", "abstract": "Three-factor learning rules in Spiking Neural Networks (SNNs) have emerged as a crucial extension to traditional Hebbian learning and Spike-Timing-Dependent Plasticity (STDP), incorporating neuromodulatory signals to improve adaptation and learning efficiency. These mechanisms enhance biological plausibility and facilitate improved credit assignment in artificial neural systems. This paper takes a view on this topic from a machine learning perspective, providing an overview of recent advances in three-factor learning, discusses theoretical foundations, algorithmic implementations, and their relevance to reinforcement learning and neuromorphic computing. In addition, we explore interdisciplinary approaches, scalability challenges, and potential applications in robotics, cognitive modeling, and AI systems. Finally, we highlight key research gaps and propose future directions for bridging the gap between neuroscience and artificial intelligence.", "field": ["Neural and Evolutionary Computing", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTM0MQ"}, {"title": "Mapping at First Sense: A Lightweight Neural Network-Based Indoor Structures Prediction Method for Robot Autonomous Exploration", "authors": ["Haojia Gao"], "date": "April 2025", "doi": "arXiv:2504.04061", "url": "https://arxiv.org/abs/2504.04061", "pdf": "https://arxiv.org/pdf/2504.04061", "abstract": "Autonomous exploration in unknown environments is a critical challenge in robotics, particularly for applications such as indoor navigation, search and rescue, and service robotics. Traditional exploration strategies, such as frontier-based methods, often struggle to efficiently utilize prior knowledge of structural regularities in indoor spaces. To address this limitation, we propose Mapping at First Sense, a lightweight neural network-based approach that predicts unobserved areas in local maps, thereby enhancing exploration efficiency. The core of our method, SenseMapNet, integrates convolutional and transformerbased architectures to infer occluded regions while maintaining computational efficiency for real-time deployment on resourceconstrained robots. Additionally, we introduce SenseMapDataset, a curated dataset constructed from KTH and HouseExpo environments, which facilitates training and evaluation of neural models for indoor exploration. Experimental results demonstrate that SenseMapNet achieves an SSIM (structural similarity) of 0.78, LPIPS (perceptual quality) of 0.68, and an FID (feature distribution alignment) of 239.79, outperforming conventional methods in map reconstruction quality. Compared to traditional frontier-based exploration, our method reduces exploration time by 46.5% (from 2335.56s to 1248.68s) while maintaining a high coverage rate (88%) and achieving a reconstruction accuracy of 88%. The proposed method represents a promising step toward efficient, learning-driven robotic exploration in structured environments.", "field": ["Robotics", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDA2MQ"}, {"title": "StayLTC: A Cost-Effective Multimodal Framework for Hospital Length of Stay Forecasting", "authors": ["Sudeshna Jana"], "date": "April 2025", "doi": "arXiv:2504.05691", "url": "https://arxiv.org/abs/2504.05691", "pdf": "https://arxiv.org/pdf/2504.05691", "abstract": "Accurate prediction of Length of Stay (LOS) in hospitals is crucial for improving healthcare services, resource management, and cost efficiency. This paper presents StayLTC, a multimodal deep learning framework developed to forecast real-time hospital LOS using Liquid Time-Constant Networks (LTCs). LTCs, with their continuous-time recurrent dynamics, are evaluated against traditional models using structured data from Electronic Health Records (EHRs) and clinical notes. Our evaluation, conducted on the MIMIC-III dataset, demonstrated that LTCs significantly outperform most of the other time series models, offering enhanced accuracy, robustness, and efficiency in resource utilization. Additionally, LTCs demonstrate a comparable performance in LOS prediction compared to time series large language models, while requiring significantly less computational power and memory, underscoring their potential to advance Natural Language Processing (NLP) tasks in healthcare.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTY5MQ"}, {"title": "Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?", "authors": ["Roman Kochnev"], "date": "April 2025", "doi": "arXiv:2504.06006", "url": "https://arxiv.org/abs/2504.06006", "pdf": "https://arxiv.org/pdf/2504.06006", "abstract": "Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of using large language models (LLMs) for hyperparameter optimization by employing a fine-tuned version of Code Llama. Through parameter-efficient fine-tuning using LoRA, we adapt the LLM to generate accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional methods such as Optuna, which rely on exhaustive trials, the proposed approach achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our approach highlights that LLM-based optimization not only matches state-of-the-art methods like Tree-structured Parzen Estimators but also accelerates the tuning process. This positions LLMs as a promising alternative to conventional optimization techniques, particularly for rapid experimentation. Furthermore, the ability to generate hyperparameters in a single inference step makes this method particularly well-suited for resource-constrained environments such as edge devices and mobile applications, where computational efficiency is paramount. The results confirm that LLMs, beyond their efficiency, offer substantial time savings and comparable stability, underscoring their value in advancing machine learning workflows. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research.", "field": ["Machine Learning", "Artificial Intelligence", "Neural and Evolutionary Computing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjAwNg"}, {"title": "Actuarial Learning for Pension Fund Mortality Forecasting", "authors": ["Eduardo Fraga L. de Melo"], "date": "April 2025", "doi": "arXiv:2504.05881", "url": "https://arxiv.org/abs/2504.05881", "pdf": "https://arxiv.org/pdf/2504.05881", "abstract": "For the assessment of the financial soundness of a pension fund, it is necessary to take into account mortality forecasting so that longevity risk is consistently incorporated into future cash flows. In this article, we employ machine learning models applied to actuarial science ({\\it actuarial learning}) to make mortality predictions for a relevant sample of pension funds' participants. Actuarial learning represents an emerging field that involves the application of machine learning (ML) and artificial intelligence (AI) techniques in actuarial science. This encompasses the use of algorithms and computational models to analyze large sets of actuarial data, such as regression trees, random forest, boosting, XGBoost, CatBoost, and neural networks (eg. FNN, LSTM, and MHA). Our results indicate that some ML/AI algorithms present competitive out-of-sample performance when compared to the classical Lee-Carter model. This may indicate interesting alternatives for consistent liability evaluation and effective pension fund risk management.", "field": ["Machine Learning", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTg4MQ"}, {"title": "Generative Artificial Intelligence for Internet of Things Computing: A Systematic Survey", "authors": ["Fabrizio Mangione"], "date": "April 2025", "doi": "arXiv:2504.07635", "url": "https://arxiv.org/abs/2504.07635", "pdf": "https://arxiv.org/pdf/2504.07635", "abstract": "The integration of Generative Artificial Intelligence (GenAI) within the Internet of Things (IoT) is garnering considerable interest. This growing attention stems from the continuous evolution and widespread adoption they are both having individually, enough to spontaneously reshape numerous sectors, including Healthcare, Manufacturing, and Smart Cities. Hence, their increasing popularity has catalyzed further extensive research for understanding the potential of the duo GenAI-IoT, how they interplay, and to which extent their synergy can innovate the state-of-the-art in their individual scenarios. However, despite the increasing prominence of GenAI for IoT Computing, much of the existing research remains focused on specific, narrowly scoped applications. This fragmented approach highlights the need for a more comprehensive analysis of the potential, challenges, and implications of GenAI integration within the broader IoT ecosystem. This survey exactly aims to address this gap by providing a holistic overview of the opportunities, issues, and considerations arising from the convergence of these mainstream paradigms. Our contribution is realized through a systematic literature review following the PRISMA methodology. A comparison framework is presented, and well-defined research questions are outlined to comprehensively explore the past, present, and future directions of GenAI integration with IoT Computing, offering valuable insights for both experts and newcomers.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzYzNQ"}, {"title": "PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning", "authors": ["Vahid Eghbal Akhlaghi"], "date": "April 2025", "doi": "arXiv:2504.07383", "url": "https://arxiv.org/abs/2504.07383", "pdf": "https://arxiv.org/pdf/2504.07383", "abstract": "This paper considers how to fuse Machine Learning (ML) and optimization to solve large-scale Supply Chain Planning (SCP) optimization problems. These problems can be formulated as MIP models which feature both integer (non-binary) and continuous variables, as well as flow balance and capacity constraints. This raises fundamental challenges for existing integrations of ML and optimization that have focused on binary MIPs and graph problems. To address these, the paper proposes PROPEL, a new framework that combines optimization with both supervised and Deep Reinforcement Learning (DRL) to reduce the size of search space significantly. PROPEL uses supervised learning, not to predict the values of all integer variables, but to identify the variables that are fixed to zero in the optimal solution, leveraging the structure of SCP applications. PROPEL includes a DRL component that selects which fixed-at-zero variables must be relaxed to improve solution quality when the supervised learning step does not produce a solution with the desired optimality tolerance. PROPEL has been applied to industrial supply chain planning optimizations with millions of variables. The computational results show dramatic improvements in solution times and quality, including a 60% reduction in primal integral and an 88% primal gap reduction, and improvement factors of up to 13.57 and 15.92, respectively.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzM4Mw"}, {"title": "Representation Meets Optimization: Training PINNs and PIKANs for Gray-Box Discovery in Systems Pharmacology", "authors": ["Nazanin Ahmadi Daryakenari"], "date": "April 2025", "doi": "arXiv:2504.07379", "url": "https://arxiv.org/abs/2504.07379", "pdf": "https://arxiv.org/pdf/2504.07379", "abstract": "Physics-Informed Kolmogorov-Arnold Networks (PIKANs) are gaining attention as an effective counterpart to the original multilayer perceptron-based Physics-Informed Neural Networks (PINNs). Both representation models can address inverse problems and facilitate gray-box system identification. However, a comprehensive understanding of their performance in terms of accuracy and speed remains underexplored. In particular, we introduce a modified PIKAN architecture, tanh-cPIKAN, which is based on Chebyshev polynomials for parametrization of the univariate functions with an extra nonlinearity for enhanced performance. We then present a systematic investigation of how choices of the optimizer, representation, and training configuration influence the performance of PINNs and PIKANs in the context of systems pharmacology modeling. We benchmark a wide range of first-order, second-order, and hybrid optimizers, including various learning rate schedulers. We use the new Optax library to identify the most effective combinations for learning gray-boxes under ill-posed, non-unique, and data-sparse conditions. We examine the influence of model architecture (MLP vs. KAN), numerical precision (single vs. double), the need for warm-up phases for second-order methods, and sensitivity to the initial learning rate. We also assess the optimizer scalability for larger models and analyze the trade-offs introduced by JAX in terms of computational efficiency and numerical accuracy. Using two representative systems pharmacology case studies - a pharmacokinetics model and a chemotherapy drug-response model - we offer practical guidance on selecting optimizers and representation models/architectures for robust and efficient gray-box discovery. Our findings provide actionable insights for improving the training of physics-informed networks in biomedical applications and beyond.", "field": ["Quantitative Methods", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzM3OQ"}, {"title": "Accelerating IoV Intrusion Detection: Benchmarking GPU-Accelerated vs CPU-Based ML Libraries", "authors": ["Furkan \u00c7olhak"], "date": "April 2025", "doi": "arXiv:2504.01905", "url": "https://arxiv.org/abs/2504.01905", "pdf": "https://arxiv.org/pdf/2504.01905", "abstract": "The Internet of Vehicles (IoV) may face challenging cybersecurity attacks that may require sophisticated intrusion detection systems, necessitating a rapid development and response system. This research investigates the performance advantages of GPU-accelerated libraries (cuML) compared to traditional CPU-based implementations (scikit-learn), focusing on the speed and efficiency required for machine learning models used in IoV threat detection environments. The comprehensive evaluations conducted employ four machine learning approaches (Random Forest, KNN, Logistic Regression, XGBoost) across three distinct IoV security datasets (OTIDS, GIDS, CICIoV2024). Our findings demonstrate that GPU-accelerated implementations dramatically improved computational efficiency, with training times reduced by a factor of up to 159 and prediction speeds accelerated by up to 95 times compared to traditional CPU processing, all while preserving detection accuracy. This remarkable performance breakthrough empowers researchers and security specialists to harness GPU acceleration for creating faster, more effective threat detection systems that meet the urgent real-time security demands of today's connected vehicle networks.", "field": ["Machine Learning", "Artificial Intelligence", "Cryptography and Security"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTkwNQ"}, {"title": "Transforming Future Data Center Operations and Management via Physical AI", "authors": ["Zhiwei Cao"], "date": "April 2025", "doi": "arXiv:2504.04982", "url": "https://arxiv.org/abs/2504.04982", "pdf": "https://arxiv.org/pdf/2504.04982", "abstract": "Data centers (DCs) as mission-critical infrastructures are pivotal in powering the growth of artificial intelligence (AI) and the digital economy. The evolution from Internet DC to AI DC has introduced new challenges in operating and managing data centers for improved business resilience and reduced total cost of ownership. As a result, new paradigms, beyond the traditional approaches based on best practices, must be in order for future data centers. In this research, we propose and develop a novel Physical AI (PhyAI) framework for advancing DC operations and management. Our system leverages the emerging capabilities of state-of-the-art industrial products and our in-house research and development. Specifically, it presents three core modules, namely: 1) an industry-grade in-house simulation engine to simulate DC operations in a highly accurate manner, 2) an AI engine built upon NVIDIA PhysicsNemo for the training and evaluation of physics-informed machine learning (PIML) models, and 3) a digital twin platform built upon NVIDIA Omniverse for our proposed 5-tier digital twin framework. This system presents a scalable and adaptable solution to digitalize, optimize, and automate future data center operations and management, by enabling real-time digital twins for future data centers. To illustrate its effectiveness, we present a compelling case study on building a surrogate model for predicting the thermal and airflow profiles of a large-scale DC in a real-time manner. Our results demonstrate its superior performance over traditional time-consuming Computational Fluid Dynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature prediction error of 0.18 \u00b0C. This emerging approach would open doors to several potential research directions for advancing Physical AI in future DC operations.", "field": ["Artificial Intelligence", "Distributed, Parallel, and Cluster Computing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDk4Mg"}, {"title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling", "authors": ["Weiqing Li"], "date": "April 2025", "doi": "arXiv:2504.03775", "url": "https://arxiv.org/abs/2504.03775", "pdf": "https://arxiv.org/pdf/2504.03775", "abstract": "Disaggregated inference has become an essential framework that separates the prefill (P) and decode (D) stages in large language model inference to improve throughput. However, the KV cache transfer faces significant delays between prefill and decode nodes. The block-wise calling method and discontinuous KV cache memory allocation increase the number of calls to the transmission kernel. Additionally, existing frameworks often fix the roles of P and D nodes, leading to computational imbalances. In this paper, we propose FlowKV, a novel disaggregated inference framework, which reduces the average transmission latency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the transfer time relative to the total request latency by optimizing the KV cache transfer. FlowKV introduces the Load-Aware Scheduler for balanced request scheduling and flexible PD node allocation. This design maximizes hardware resource utilization, achieving peak system throughput across various scenarios, including normal, computational imbalance, and extreme overload conditions. Experimental results demonstrate that FlowKV significantly accelerates inference by 15.2%-48.9% on LongBench dataset compared to the baseline and supports applications with heterogeneous GPUs.", "field": ["Distributed, Parallel, and Cluster Computing", "Artificial Intelligence", "Computation and Language"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzc3NQ"}, {"title": "Exploring energy consumption of AI frameworks on a 64-core RV64 Server CPU", "authors": ["Giulio Malenza"], "date": "April 2025", "doi": "arXiv:2504.03774", "url": "https://arxiv.org/abs/2504.03774", "pdf": "https://arxiv.org/pdf/2504.03774", "abstract": "In today's era of rapid technological advancement, artificial intelligence (AI) applications require large-scale, high-performance, and data-intensive computations, leading to significant energy demands. Addressing this challenge necessitates a combined approach involving both hardware and software innovations. Hardware manufacturers are developing new, efficient, and specialized solutions, with the RISC-V architecture emerging as a prominent player due to its open, extensible, and energy-efficient instruction set architecture (ISA). Simultaneously, software developers are creating new algorithms and frameworks, yet their energy efficiency often remains unclear. In this study, we conduct a comprehensive benchmark analysis of machine learning (ML) applications on the 64-core SOPHON SG2042 RISC-V architecture. We specifically analyze the energy consumption of deep learning inference models across three leading AI frameworks: PyTorch, ONNX Runtime, and TensorFlow. Our findings show that frameworks using the XNNPACK back-end, such as ONNX Runtime and TensorFlow, consume less energy compared to PyTorch, which is compiled with the native OpenBLAS back-end.", "field": ["Distributed, Parallel, and Cluster Computing", "Artificial Intelligence", "Hardware Architecture"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzc3NA"}, {"title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models", "authors": ["Afshin Khadangi"], "date": "April 2025", "doi": "arXiv:2504.03302", "url": "https://arxiv.org/abs/2504.03302", "pdf": "https://arxiv.org/pdf/2504.03302", "abstract": "Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzMwMg"}, {"title": "Multi-Granularity Vision Fastformer with Fusion Mechanism for Skin Lesion Segmentation", "authors": ["Xuanyu Liu"], "date": "April 2025", "doi": "arXiv:2504.03108", "url": "https://arxiv.org/abs/2504.03108", "pdf": "https://arxiv.org/pdf/2504.03108", "abstract": "Background:Convolutional Neural Networks(CNN) and Vision Transformers(ViT) are the main techniques used in Medical image segmentation. However, CNN is limited to local contextual information, and ViT's quadratic complexity results in significant computational costs. At the same time, equipping the model to distinguish lesion boundaries with varying degrees of severity is also a challenge encountered in skin lesion segmentation. Purpose:This research aims to optimize the balance between computational costs and long-range dependency modelling and achieve excellent generalization across lesions with different degrees of severity. Methods:we propose a lightweight U-shape network that utilizes Vision Fastformer with Fusion Mechanism (VFFM-UNet). We inherit the advantages of Fastformer's additive attention mechanism, combining element-wise product and matrix product for comprehensive feature extraction and channel reduction to save computational costs. In order to accurately identify the lesion boundaries with varying degrees of severity, we designed Fusion Mechanism including Multi-Granularity Fusion and Channel Fusion, which can process the feature maps in the granularity and channel levels to obtain different contextual information. Results:Comprehensive experiments on the ISIC2017, ISIC2018 and PH2 datasets demonstrate that VFFM-UNet outperforms existing state-of-the-art models regarding parameter numbers, computational complexity and segmentation performance. In short, compared to MISSFormer, our model achieves superior segmentation performance while reducing parameter and computation costs by 101x and 15x, respectively. Conclusions:Both quantitative and qualitative analyses show that VFFM-UNet sets a new benchmark by reaching an ideal balance between parameter numbers, computational complexity, and segmentation performance compared to existing state-of-the-art models.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzEwOA"}, {"title": "Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems", "authors": ["Bang Liu"], "date": "April 2025", "doi": "arXiv:2504.01990", "url": "https://arxiv.org/abs/2504.01990", "pdf": "https://arxiv.org/pdf/2504.01990", "abstract": "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This survey provides a comprehensive overview, framing intelligent agents within a modular, brain-inspired architecture that integrates principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we delve into the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities, and elucidating core components such as memory, world modeling, reward processing, and emotion-like systems. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms, including emerging AutoML and LLM-driven optimization strategies. Third, we examine collaborative and evolutionary multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures, highlighting parallels to human social dynamics. Finally, we address the critical imperative of building safe, secure, and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTk5MA"}, {"title": "A novel numerical method tailored for unconstrained optimization problems", "authors": ["Lin Li"], "date": "April 2025", "doi": "arXiv:2504.02832", "url": "https://arxiv.org/abs/2504.02832", "pdf": "https://arxiv.org/pdf/2504.02832", "abstract": "Unconstrained optimization problems become more common in scientific computing and engineering applications with the rapid development of artificial intelligence, and numerical methods for solving them more quickly and efficiently have been getting more attention and research. Moreover, an efficient method to minimize all kinds of objective functions is urgently needed, especially the nonsmooth objective function. Therefore, in the current paper, we focus on proposing a novel numerical method tailored for unconstrained optimization problems whether the objective function is smooth or not. To be specific, based on the variational procedure to refine the gradient and Hessian matrix approximations, an efficient quadratic model with $2n$ constrained conditions is established. Moreover, to improve the computational efficiency, a simplified model with 2 constrained conditions is also proposed, where the gradient and Hessian matrix can be explicitly updated, and the corresponding boundedness of the remaining $2n-2$ constrained conditions is derived. On the other hand, the novel numerical method is summarized, and approximation results on derivative information are also analyzed and shown. Numerical experiments involving smooth, derivative blasting, and non-smooth problems are tested, demonstrating its feasibility and efficiency. Compared with existing methods, our proposed method can efficiently solve smooth and non-smooth unconstrained optimization problems for the first time, and it is very easy to program the code, indicating that our proposed method not also has great application prospects, but is also very meaningful to explore practical complex engineering and scientific problems.", "field": ["Optimization and Control", "Numerical Analysis"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjgzMg"}, {"title": "Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning", "authors": ["Yinggan Xu"], "date": "April 2025", "doi": "arXiv:2504.01911", "url": "https://arxiv.org/abs/2504.01911", "pdf": "https://arxiv.org/pdf/2504.01911", "abstract": "Large Language Models (LLMs) are playing an expanding role in physics research by enhancing reasoning, symbolic manipulation, and numerical computation. However, ensuring the reliability and interpretability of their outputs remains a significant challenge. In our framework, we conceptualize the collaboration between AI and human scientists as a dynamic interplay among three modules: the reasoning module, the interpretation module, and the AI-scientist interaction module. Recognizing that effective physics reasoning demands rigorous logical consistency, quantitative precision, and deep integration with established theoretical models, we introduce the interpretation module to improve the understanding of AI-generated outputs, which is not previously explored in the literature. This module comprises multiple specialized agents, including summarizers, model builders, UI builders, and testers, which collaboratively structure LLM outputs within a physically grounded framework, by constructing a more interpretable science model. A case study demonstrates that our approach enhances transparency, facilitates validation, and strengthens AI-augmented reasoning in scientific discovery.", "field": ["Artificial Intelligence", "Computation and Language", "Human-Computer Interaction", "Computational Physics"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTkxMQ"}, {"title": "Introducing COGENT3: An AI Architecture for Emergent Cognition", "authors": ["Eduardo Salazar"], "date": "April 2025", "doi": "arXiv:2504.04139", "url": "https://arxiv.org/abs/2504.04139", "pdf": "https://arxiv.org/pdf/2504.04139", "abstract": "This paper presents COGENT3 (or Collective Growth and Entropy-modulated Triads System), a novel approach for emergent cognition integrating pattern formation networks with group influence dynamics. Contrasting with traditional strategies that rely on predetermined architectures, computational structures emerge dynamically in our framework through agent interactions. This enables a more flexible and adaptive system exhibiting characteristics reminiscent of human cognitive processes. The incorporation of temperature modulation and memory effects in COGENT3 closely integrates statistical mechanics, machine learning, and cognitive science.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDEzOQ"}, {"title": "VFlow: Discovering Optimal Agentic Workflows for Verilog Generation", "authors": ["Yangbo Wei"], "date": "April 2025", "doi": "arXiv:2504.03723", "url": "https://arxiv.org/abs/2504.03723", "pdf": "https://arxiv.org/pdf/2504.03723", "abstract": "Hardware design automation faces challenges in generating high-quality Verilog code efficiently. This paper introduces VFlow, an automated framework that optimizes agentic workflows for Verilog code generation. Unlike existing approaches that rely on pre-defined prompting strategies, VFlow leverages Monte Carlo Tree Search (MCTS) to discover effective sequences of Large Language Models invocations that maximize code quality while minimizing computational costs. VFlow extends the AFLOW methodology with domain-specific operators addressing hardware design requirements, including syntax validation, simulation-based verification, and synthesis optimization. Experimental evaluation on the VerilogEval benchmark demonstrates VFlow's superiority, achieving an 83.6% average pass@1 rate-a 6.1\\% improvement over state-of-the-art PromptV and a 36.9\\% gain compared to direct LLM invocation. Most significantly, VFlow enhances the capabilities of smaller models, enabling DeepSeek-V3 to achieve 141.2\\% of GPT-4o's performance while reducing API costs to just 13\\%. These findings indicate that intelligently optimized workflows enable cost-efficient LLMs to outperform larger models on hardware design tasks, potentially democratizing access to advanced digital circuit development tools and accelerating innovation in the semiconductor industry", "field": ["Hardware Architecture", "Multiagent Systems"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzcyMw"}, {"title": "Solving AI Foundational Model Latency with Telco Infrastructure", "authors": ["Sebastian Barros"], "date": "April 2025", "doi": "arXiv:2504.03708", "url": "https://arxiv.org/abs/2504.03708", "pdf": "https://arxiv.org/pdf/2504.03708", "abstract": "Latency remains a critical bottleneck for deploying foundational artificial intelligence (AI) models, such as large language models (LLMs), in customer-facing, real-time applications. While cloud-based inference offers scalability, it frequently introduces delays unacceptable for interactive experiences, such as semantic search, personalized recommendations, or conversational interfaces. Telecommunications operators, historically adept at solving content latency challenges through partnerships with providers like Google and Facebook, now have a unique opportunity to address similar AI latency concerns. This paper presents a technical framework leveraging Telco infrastructure-spanning regional data centers, existing content delivery network (CDN) nodes, and near-radio access network (RAN) sites-as hierarchical \"AI edges\" for caching and partial inference. We explore the architectural feasibility of embedding semantic and vector-based AI inference caches within existing Telco assets, proposing tiered caching strategies and split-inference architectures that significantly reduce latency and compute costs. Additionally, we address technical challenges specific to Telcos, such as cache synchronization, model distribution, privacy, and hardware acceleration considerations. Finally, we discuss viable partnership models between telcos and AI providers, highlighting how this innovative use of telco infrastructure can unlock both improved AI user experience and new revenue streams.", "field": ["Networking and Internet Architecture", "Distributed, Parallel, and Cluster Computing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzcwOA"}, {"title": "Intelligent Resource Allocation Optimization for Cloud Computing via Machine Learning", "authors": ["Yuqing Wang"], "date": "April 2025", "doi": "arXiv:2504.03682", "url": "https://arxiv.org/abs/2504.03682", "pdf": "https://arxiv.org/pdf/2504.03682", "abstract": "With the rapid expansion of cloud computing applications, optimizing resource allocation has become crucial for improving system performance and cost efficiency. This paper proposes an intelligent resource allocation algorithm that leverages deep learning (LSTM) for demand prediction and reinforcement learning (DQN) for dynamic scheduling. By accurately forecasting computing resource demands and enabling real-time adjustments, the proposed system enhances resource utilization by 32.5%, reduces average response time by 43.3%, and lowers operational costs by 26.6%. Experimental results in a production cloud environment confirm that the method significantly improves efficiency while maintaining high service quality. This study provides a scalable and effective solution for intelligent cloud resource management, offering valuable insights for future cloud optimization strategies.", "field": ["Distributed, Parallel, and Cluster Computing", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzY4Mg"}, {"title": "Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment", "authors": ["Caicheng Wang"], "date": "April 2025", "doi": "arXiv:2504.03669", "url": "https://arxiv.org/abs/2504.03669", "pdf": "https://arxiv.org/pdf/2504.03669", "abstract": "Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.", "field": ["Machine Learning", "Artificial Intelligence", "Systems and Control"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzY2OQ"}, {"title": "AI2STOW: End-to-End Deep Reinforcement Learning to Construct Master Stowage Plans under Demand Uncertainty", "authors": ["Jaike Van Twiller"], "date": "April 2025", "doi": "arXiv:2504.04469", "url": "https://arxiv.org/abs/2504.04469", "pdf": "https://arxiv.org/pdf/2504.04469", "abstract": "The worldwide economy and environmental sustainability depend on eff icient and reliable supply chains, in which container shipping plays a crucial role as an environmentally friendly mode of transport. Liner shipping companies seek to improve operational efficiency by solving the stowage planning problem. Due to many complex combinatorial aspects, stowage planning is challenging and often decomposed into two NP-hard subproblems: master and slot planning. This article proposes AI2STOW, an end-to-end deep reinforcement learning model with feasibility projection and an action mask to create master plans under demand uncertainty with global objectives and constraints, including paired block stowage patterms. Our experimental results demonstrate that AI2STOW outperforms baseline methods from reinforcement learning and stochastic programming in objective performance and computational efficiency, based on simulated instances reflecting the scale of realistic vessels and operational planning horizons.", "field": ["Optimization and Control", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDQ2OQ"}, {"title": "AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence", "authors": ["Matej \u0160progar"], "date": "April 2025", "doi": "arXiv:2504.04430", "url": "https://arxiv.org/abs/2504.04430", "pdf": "https://arxiv.org/pdf/2504.04430", "abstract": "Despite remarkable progress in machine learning, current AI systems continue to fall short of true human-like intelligence. While Large Language Models (LLMs) excel in pattern recognition and response generation, they lack genuine understanding - an essential hallmark of Artificial General Intelligence (AGI). Existing AGI evaluation methods fail to offer a practical, gradual, and informative metric. This paper introduces the Artificial General Intelligence Test Bed (AGITB), comprising twelve rigorous tests that form a signal-processing-level foundation for the potential emergence of cognitive capabilities. AGITB evaluates intelligence through a model's ability to predict binary signals across time without relying on symbolic representations or pretraining. Unlike high-level tests grounded in language or perception, AGITB focuses on core computational invariants reflective of biological intelligence, such as determinism, sensitivity, and generalisation. The test bed assumes no prior bias, operates independently of semantic meaning, and ensures unsolvability through brute force or memorization. While humans pass AGITB by design, no current AI system has met its criteria, making AGITB a compelling benchmark for guiding and recognizing progress toward AGI.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDQzMA"}, {"title": "Future-Proof Yourself: An AI Era Survival Guide", "authors": ["Taehoon Kim"], "date": "April 2025", "doi": "arXiv:2504.04378", "url": "https://arxiv.org/abs/2504.04378", "pdf": "https://arxiv.org/pdf/2504.04378", "abstract": "Future-Proof Yourself is a practical guide that helps readers navigate the fast-changing world of artificial intelligence in everyday life. The book begins by explaining how computers learn from data in simple, relatable terms, and gradually introduces the methods used in modern AI. It shows how basic ideas in machine learning evolve into advanced systems that can recognize images, understand language, and even make decisions. The guide also reviews the history of AI and highlights the major breakthroughs that have shaped its growth. Looking ahead, the book explores emerging trends such as the integration of AI with digital twins, wearable devices, and virtual environments. Designed for a general audience, the text avoids heavy technical jargon and presents complex ideas in clear, straightforward language so that anyone can gain a solid understanding of the technology that is set to transform our future.", "field": ["Machine Learning", "Artificial Intelligence", "Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDM3OA"}, {"title": "A Comparative Study of Explainable AI Methods: Model-Agnostic vs. Model-Specific Approaches", "authors": ["Keerthi Devireddy"], "date": "April 2025", "doi": "arXiv:2504.04276", "url": "https://arxiv.org/abs/2504.04276", "pdf": "https://arxiv.org/pdf/2504.04276", "abstract": "This paper compares model-agnostic and model-specific approaches to explainable AI (XAI) in deep learning image classification. I examine how LIME and SHAP (model-agnostic methods) differ from Grad-CAM and Guided Backpropagation (model-specific methods) when interpreting ResNet50 predictions across diverse image categories. Through extensive testing with various species from dogs and birds to insects I found that each method reveals different aspects of the models decision-making process. Model-agnostic techniques provide broader feature attribution that works across different architectures, while model-specific approaches excel at highlighting precise activation regions with greater computational efficiency. My analysis shows there is no \"one-size-fits-all\" solution for model interpretability. Instead, combining multiple XAI methods offers the most comprehensive understanding of complex models particularly valuable in high-stakes domains like healthcare, autonomous vehicles, and financial services where transparency is crucial. This comparative framework provides practical guidance for selecting appropriate interpretability techniques based on specific application needs and computational constraints.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDI3Ng"}, {"title": "TuRTLe: A Unified Evaluation of LLMs for RTL Generation", "authors": ["Dario Garcia-Gasulla"], "date": "April 2025", "doi": "arXiv:2504.01986", "url": "https://arxiv.org/abs/2504.01986", "pdf": "https://arxiv.org/pdf/2504.01986", "abstract": "The rapid advancements in LLMs have driven the adoption of generative AI in various domains, including Electronic Design Automation (EDA). Unlike traditional software development, EDA presents unique challenges, as generated RTL code must not only be syntactically correct and functionally accurate but also synthesizable by hardware generators while meeting performance, power, and area constraints. These additional requirements introduce complexities that existing code-generation benchmarks often fail to capture, limiting their effectiveness in evaluating LLMs for RTL generation. To address this gap, we propose TuRTLe, a unified evaluation framework designed to systematically assess LLMs across key RTL generation tasks. TuRTLe integrates multiple existing benchmarks and automates the evaluation process, enabling a comprehensive assessment of LLM performance in syntax correctness, functional correctness, synthesis, PPA optimization, and exact line completion. Using this framework, we benchmark a diverse set of open LLMs and analyze their strengths and weaknesses in EDA-specific tasks. Our results show that reasoning-based models, such as DeepSeek R1, consistently outperform others across multiple evaluation criteria, but at the cost of increased computational overhead and inference latency. Additionally, base models are better suited in module completion tasks, while instruct-tuned models perform better in specification-to-RTL tasks.", "field": ["Hardware Architecture", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTk4Ng"}, {"title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning", "authors": ["Leonardo Iurada"], "date": "April 2025", "doi": "arXiv:2504.02620", "url": "https://arxiv.org/abs/2504.02620", "pdf": "https://arxiv.org/pdf/2504.02620", "abstract": "Task arithmetic has emerged as a promising approach for editing models by representing task-specific knowledge as composable task vectors. However, existing methods rely on network linearization to derive task vectors, leading to computational bottlenecks during training and inference. Moreover, linearization alone does not ensure weight disentanglement, the key property that enables conflict-free composition of task vectors. To address this, we propose TaLoS which allows to build sparse task vectors with minimal interference without requiring explicit linearization and sharing information across tasks. We find that pre-trained models contain a subset of parameters with consistently low gradient sensitivity across tasks, and that sparsely updating only these parameters allows for promoting weight disentanglement during fine-tuning. Our experiments prove that TaLoS improves training and inference efficiency while outperforming current methods in task addition and negation. By enabling modular parameter editing, our approach fosters practical deployment of adaptable foundation models in real-world applications.", "field": ["Machine Learning", "Artificial Intelligence", "Computation and Language", "Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjYyMA"}, {"title": "GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning", "authors": ["Xiangxiang Chu"], "date": "April 2025", "doi": "arXiv:2504.02546", "url": "https://arxiv.org/abs/2504.02546", "pdf": "https://arxiv.org/pdf/2504.02546", "abstract": "Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. As illustrated in our paper, by eliminating both the critic and reference models, and avoiding KL divergence constraints, our approach significantly simplifies the training process when compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. Extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjU0Ng"}, {"title": "LLMPi: Optimizing LLMs for High-Throughput on Raspberry Pi", "authors": ["Mahsa Ardakani"], "date": "April 2025", "doi": "arXiv:2504.02118", "url": "https://arxiv.org/abs/2504.02118", "pdf": "https://arxiv.org/pdf/2504.02118", "abstract": "Deploying Large Language Models (LLMs) on resource-constrained edge devices like the Raspberry Pi presents challenges in computational efficiency, power consumption, and response latency. This paper explores quantization-based optimization techniques to enable high-throughput, energy-efficient execution of LLMs on low-power embedded systems. Our approach leverages k-quantization, a Post-Training Quantization (PTQ) method designed for different bit-widths, enabling efficient 2-bit, 4-bit, 6-bit, and 8-bit weight quantization. Additionally, we employ ternary quantization using Quantization-Aware Training (QAT) for BitNet models, allowing for more effective adaptation to lower-bit representations while preserving accuracy.\n  Our findings highlight the potential of quantized LLMs for real-time conversational AI on edge devices, paving the way for low-power, high-efficiency AI deployment in mobile and embedded applications. This study demonstrates that aggressive quantization strategies can significantly reduce energy consumption while maintaining inference quality, making LLMs practical for resource-limited environments.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjExOA"}, {"title": "Increasing happiness through conversations with artificial intelligence", "authors": ["Joseph Heffner"], "date": "April 2025", "doi": "arXiv:2504.02091", "url": "https://arxiv.org/abs/2504.02091", "pdf": "https://arxiv.org/pdf/2504.02091", "abstract": "Chatbots powered by artificial intelligence (AI) have rapidly become a significant part of everyday life, with over a quarter of American adults using them multiple times per week. While these tools offer potential benefits and risks, a fundamental question remains largely unexplored: How do conversations with AI influence subjective well-being? To investigate this, we conducted a study where participants either engaged in conversations with an AI chatbot (N = 334) or wrote journal entires (N = 193) on the same randomly assigned topics and reported their momentary happiness afterward. We found that happiness after AI chatbot conversations was higher than after journaling, particularly when discussing negative topics such as depression or guilt. Leveraging large language models for sentiment analysis, we found that the AI chatbot mirrored participants' sentiment while maintaining a consistent positivity bias. When discussing negative topics, participants gradually aligned their sentiment with the AI's positivity, leading to an overall increase in happiness. We hypothesized that the history of participants' sentiment prediction errors, the difference between expected and actual emotional tone when responding to the AI chatbot, might explain this happiness effect. Using computational modeling, we find the history of these sentiment prediction errors over the course of a conversation predicts greater post-conversation happiness, demonstrating a central role of emotional expectations during dialogue. Our findings underscore the effect that AI interactions can have on human well-being.", "field": ["Computation and Language"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjA5MQ"}, {"title": "Test-time Adaptation for Foundation Medical Segmentation Model without Parametric Updates", "authors": ["Kecheng Chen"], "date": "April 2025", "doi": "arXiv:2504.02008", "url": "https://arxiv.org/abs/2504.02008", "pdf": "https://arxiv.org/pdf/2504.02008", "abstract": "Foundation medical segmentation models, with MedSAM being the most popular, have achieved promising performance across organs and lesions. However, MedSAM still suffers from compromised performance on specific lesions with intricate structures and appearance, as well as bounding box prompt-induced perturbations. Although current test-time adaptation (TTA) methods for medical image segmentation may tackle this issue, partial (e.g., batch normalization) or whole parametric updates restrict their effectiveness due to limited update signals or catastrophic forgetting in large models. Meanwhile, these approaches ignore the computational complexity during adaptation, which is particularly significant for modern foundation models. To this end, our theoretical analyses reveal that directly refining image embeddings is feasible to approach the same goal as parametric updates under the MedSAM architecture, which enables us to realize high computational efficiency and segmentation performance without the risk of catastrophic forgetting. Under this framework, we propose to encourage maximizing factorized conditional probabilities of the posterior prediction probability using a proposed distribution-approximated latent conditional random field loss combined with an entropy minimization loss. Experiments show that we achieve about 3\\% Dice score improvements across three datasets while reducing computational complexity by over 7 times.", "field": ["Quantitative Methods", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjAwOA"}, {"title": "Industrial Internet Robot Collaboration System and Edge Computing Optimization", "authors": ["Qian Zuo"], "date": "April 2025", "doi": "arXiv:2504.02492", "url": "https://arxiv.org/abs/2504.02492", "pdf": "https://arxiv.org/pdf/2504.02492", "abstract": "In a complex environment, for a mobile robot to safely and collision - free avoid all obstacles, it poses high requirements for its intelligence level. Given that the information such as the position and geometric characteristics of obstacles is random, the control parameters of the robot, such as velocity and angular velocity, are also prone to random deviations. To address this issue in the framework of the Industrial Internet Robot Collaboration System, this paper proposes a global path control scheme for mobile robots based on deep learning. First of all, the dynamic equation of the mobile robot is established. According to the linear velocity and angular velocity of the mobile robot, its motion behaviors are divided into obstacle - avoidance behavior, target - turning behavior, and target approaching behavior. Subsequently, the neural network method in deep learning is used to build a global path planning model for the robot. On this basis, a fuzzy controller is designed with the help of a fuzzy control algorithm to correct the deviations that occur during path planning, thereby achieving optimized control of the robot's global path. In addition, considering edge computing optimization, the proposed model can process local data at the edge device, reducing the communication burden between the robot and the central server, and improving the real time performance of path planning. The experimental results show that for the mobile robot controlled by the research method in this paper, the deviation distance of the path angle is within 5 cm, the deviation convergence can be completed within 10 ms, and the planned path is shorter. This indicates that the proposed scheme can effectively improve the global path planning ability of mobile robots in the industrial Internet environment and promote the collaborative operation of robots through edge computing optimization.", "field": ["Robotics", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjQ5Mg"}, {"title": "The quasi-semantic competence of LLMs: a case study on the part-whole relation", "authors": ["Mattia Proietti"], "date": "April 2025", "doi": "arXiv:2504.02395", "url": "https://arxiv.org/abs/2504.02395", "pdf": "https://arxiv.org/pdf/2504.02395", "abstract": "Understanding the extent and depth of the semantic competence of \\emph{Large Language Models} (LLMs) is at the center of the current scientific agenda in Artificial Intelligence (AI) and Computational Linguistics (CL). We contribute to this endeavor by investigating their knowledge of the \\emph{part-whole} relation, a.k.a. \\emph{meronymy}, which plays a crucial role in lexical organization, but it is significantly understudied. We used data from ConceptNet relations \\citep{speer2016conceptnet} and human-generated semantic feature norms \\citep{McRae:2005} to explore the abilities of LLMs to deal with \\textit{part-whole} relations. We employed several methods based on three levels of analysis: i.) \\textbf{behavioral} testing via prompting, where we directly queried the models on their knowledge of meronymy, ii.) sentence \\textbf{probability} scoring, where we tested models' abilities to discriminate correct (real) and incorrect (asymmetric counterfactual) \\textit{part-whole} relations, and iii.) \\textbf{concept representation} analysis in vector space, where we proved the linear organization of the \\textit{part-whole} concept in the embedding and unembedding spaces. These analyses present a complex picture that reveals that the LLMs' knowledge of this relation is only partial. They have just a ``\\emph{quasi}-semantic'' competence and still fall short of capturing deep inferential properties.", "field": ["Computation and Language"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjM5NQ"}, {"title": "OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning", "authors": ["Daniel Tcheurekdjian"], "date": "April 2025", "doi": "arXiv:2504.06538", "url": "https://arxiv.org/abs/2504.06538", "pdf": "https://arxiv.org/pdf/2504.06538", "abstract": "We present OPAL (Operant Physical Agent with Language), a novel vision-language-action architecture that introduces topological constraints to flow matching for robotic control. To do so, we further introduce topological attention. Our approach models action sequences as topologically-structured representations with non-trivial constraints. Experimental results across 10 complex manipulation tasks demonstrate OPAL's superior performance compared to previous approaches, including Octo, OpenVLA, and $\u03c0$0.\n  Our architecture achieves significant improvements in zero-shot performance without requiring task-specific fine-tuning, while reducing inference computational requirements by 42%. The theoretical guarantees provided by our topological approach result in more coherent long-horizon action sequences. Our results highlight the potential of constraining the search space of learning problems in robotics by deriving from fundamental physical laws, and the possibility of using topological attention to embed causal understanding into transformer architectures.", "field": ["Robotics", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjUzOA"}, {"title": "JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model", "authors": ["Yi Nian"], "date": "April 2025", "doi": "arXiv:2504.03770", "url": "https://arxiv.org/abs/2504.03770", "pdf": "https://arxiv.org/pdf/2504.03770", "abstract": "Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.", "field": ["Cryptography and Security", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzc3MA"}, {"title": "Adaptive Computation Pruning for the Forgetting Transformer", "authors": ["Zhixuan Lin"], "date": "April 2025", "doi": "arXiv:2504.06949", "url": "https://arxiv.org/abs/2504.06949", "pdf": "https://arxiv.org/pdf/2504.06949", "abstract": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.", "field": ["Machine Learning", "Artificial Intelligence", "Computation and Language"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjk0OQ"}, {"title": "Satellite Edge Artificial Intelligence with Large Models: Architectures and Technologies", "authors": ["Yuanming Shi"], "date": "April 2025", "doi": "arXiv:2504.01676", "url": "https://arxiv.org/abs/2504.01676", "pdf": "https://arxiv.org/pdf/2504.01676", "abstract": "Driven by the growing demand for intelligent remote sensing applications, large artificial intelligence (AI) models pre-trained on large-scale unlabeled datasets and fine-tuned for downstream tasks have significantly improved learning performance for various downstream tasks due to their generalization capabilities. However, many specific downstream tasks, such as extreme weather nowcasting (e.g., downburst and tornado), disaster monitoring, and battlefield surveillance, require real-time data processing. Traditional methods via transferring raw data to ground stations for processing often cause significant issues in terms of latency and trustworthiness. To address these challenges, satellite edge AI provides a paradigm shift from ground-based to on-board data processing by leveraging the integrated communication-and-computation capabilities in space computing power networks (Space-CPN), thereby enhancing the timeliness, effectiveness, and trustworthiness for remote sensing downstream tasks. Moreover, satellite edge large AI model (LAM) involves both the training (i.e., fine-tuning) and inference phases, where a key challenge lies in developing computation task decomposition principles to support scalable LAM deployment in resource-constrained space networks with time-varying topologies. In this article, we first propose a satellite federated fine-tuning architecture to split and deploy the modules of LAM over space and ground networks for efficient LAM fine-tuning. We then introduce a microservice-empowered satellite edge LAM inference architecture that virtualizes LAM components into lightweight microservices tailored for multi-task multimodal inference. Finally, we discuss the future directions for enhancing the efficiency and scalability of satellite edge LAM, including task-oriented communication, brain-inspired computing, and satellite edge AI network optimization.", "field": ["Machine Learning", "Distributed, Parallel, and Cluster Computing", "Networking and Internet Architecture", "Signal Processing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTY3Ng"}, {"title": "Near-energy-free Photonic Fourier Transformation for Convolution Operation Acceler", "authors": ["Hangbo Yang"], "date": "April 2025", "doi": "arXiv:2504.01117", "url": "https://arxiv.org/abs/2504.01117", "pdf": "https://arxiv.org/pdf/2504.01117", "abstract": "Convolutional operations are computationally intensive in artificial intelligence services, and their overhead in electronic hardware limits machine learning scaling. Here, we introduce a photonic joint transform correlator (pJTC) using a near-energy-free on-chip Fourier transformation to accelerate convolution operations. The pJTC reduces computational complexity for both convolution and cross-correlation from O(N4) to O(N2), where N2 is the input data size. Demonstrating functional Fourier transforms and convolution, this pJTC achieves 98.0% accuracy on an exemplary MNIST inference task. Furthermore, a wavelength-multiplexed pJTC architecture shows potential for high throughput and energy efficiency, reaching 305 TOPS/W and 40.2 TOPS/mm2, based on currently available foundry processes. An efficient, compact, and low-latency convolution accelerator promises to advance next-generation AI capabilities across edge demands, high-performance computing, and cloud services.", "field": ["Optics", "Computational Physics"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTExNw"}, {"title": "Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model", "authors": ["Jincheng Zhong"], "date": "April 2025", "doi": "arXiv:2504.01521", "url": "https://arxiv.org/abs/2504.01521", "pdf": "https://arxiv.org/pdf/2504.01521", "abstract": "Recent advancements in diffusion models have revolutionized generative modeling. However, the impressive and vivid outputs they produce often come at the cost of significant model scaling and increased computational demands. Consequently, building personalized diffusion models based on off-the-shelf models has emerged as an appealing alternative. In this paper, we introduce a novel perspective on conditional generation for transferring a pre-trained model. From this viewpoint, we propose *Domain Guidance*, a straightforward transfer approach that leverages pre-trained knowledge to guide the sampling process toward the target domain. Domain Guidance shares a formulation similar to advanced classifier-free guidance, facilitating better domain alignment and higher-quality generations. We provide both empirical and theoretical analyses of the mechanisms behind Domain Guidance. Our experimental results demonstrate its substantial effectiveness across various transfer benchmarks, achieving over a 19.6% improvement in FID and a 23.4% improvement in FD$_\\text{DINOv2}$ compared to standard fine-tuning. Notably, existing fine-tuned models can seamlessly integrate Domain Guidance to leverage these benefits, without additional training.", "field": ["Machine Learning", "Artificial Intelligence", "Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTUyMQ"}, {"title": "Versatile silicon integrated photonic processor: a reconfigurable solution for netx-generation AI clusters", "authors": ["Ying Zhu"], "date": "April 2025", "doi": "arXiv:2504.01463", "url": "https://arxiv.org/abs/2504.01463", "pdf": "https://arxiv.org/pdf/2504.01463", "abstract": "The Artificial Intelligence models pose serious challenges in intensive computing and high-bandwidth communication for conventional electronic circuit-based computing clusters. Silicon photonic technologies, owing to their high speed, low latency, large bandwidth, and complementary metal-oxide-semiconductor compatibility, have been widely implemented for data transfer and actively explored as photonic neural networks in AI clusters. However, current silicon photonic integrated chips lack adaptability for multifuncional use and hardware-software systematic coordination. Here, we develop a reconfigurable silicon photonic processor with $40$ programmable unit cells integrating over $160$ component, which, to the best of our knowledge, is the first to realize diverse functions with a chip for AI clusters, from computing acceleration and signal processing to network swtiching and secure encryption. Through a self-developed automated testing, compilation, and tuning framework to the processor without in-network monitoring photodetectors, we implement $4\\times4$ dual-direction unitary and $3\\times3$ uni-direction non-unitary matrix multiplications, neural networks for image recognition, micro-ring modulator wavelength locking, $4\\times4$ photonic channel switching , and silicon photonic physical unclonable functions. This optoelectronic processing system, incorporating the photonic processor and its software stack, paves the way for both advanced photonic system-on-chip design and the construction of photo-electronic AI clusters.", "field": ["Optics", "Hardware Architecture"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTQ2Mw"}, {"title": "CFMD: Dynamic Cross-layer Feature Fusion for Salient Object Detection", "authors": ["Jin Lian"], "date": "April 2025", "doi": "arXiv:2504.01326", "url": "https://arxiv.org/abs/2504.01326", "pdf": "https://arxiv.org/pdf/2504.01326", "abstract": "Cross-layer feature pyramid networks (CFPNs) have achieved notable progress in multi-scale feature fusion and boundary detail preservation for salient object detection. However, traditional CFPNs still suffer from two core limitations: (1) a computational bottleneck caused by complex feature weighting operations, and (2) degraded boundary accuracy due to feature blurring in the upsampling process. To address these challenges, we propose CFMD, a novel cross-layer feature pyramid network that introduces two key innovations. First, we design a context-aware feature aggregation module (CFLMA), which incorporates the state-of-the-art Mamba architecture to construct a dynamic weight distribution mechanism. This module adaptively adjusts feature importance based on image context, significantly improving both representation efficiency and generalization. Second, we introduce an adaptive dynamic upsampling unit (CFLMD) that preserves spatial details during resolution recovery. By adjusting the upsampling range dynamically and initializing with a bilinear strategy, the module effectively reduces feature overlap and maintains fine-grained boundary structures. Extensive experiments on three standard benchmarks using three mainstream backbone networks demonstrate that CFMD achieves substantial improvements in pixel-level accuracy and boundary segmentation quality, especially in complex scenes. The results validate the effectiveness of CFMD in jointly enhancing computational efficiency and segmentation performance, highlighting its strong potential in salient object detection tasks.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTMyNg"}, {"title": "Multi-Modality Sensing in mmWave Beamforming for Connected Vehicles Using Deep Learning", "authors": ["Muhammad Baqer Mollah"], "date": "April 2025", "doi": "arXiv:2504.06173", "url": "https://arxiv.org/abs/2504.06173", "pdf": "https://arxiv.org/pdf/2504.06173", "abstract": "Beamforming techniques are considered as essential parts to compensate severe path losses in millimeter-wave (mmWave) communications. In particular, these techniques adopt large antenna arrays and formulate narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over narrow beams for efficient link configuration by traditional standard defined beam selection approaches, which mainly rely on channel state information and beam sweeping through exhaustive searching, imposes computational and communications overheads. And, such resulting overheads limit their potential use in vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communications involving highly dynamic scenarios. In comparison, utilizing out-of-band contextual information, such as sensing data obtained from sensor devices, provides a better alternative to reduce overheads. This paper presents a deep learning-based solution for utilizing the multi-modality sensing data for predicting the optimal beams having sufficient mmWave received powers so that the best V2I and V2V line-of-sight links can be ensured proactively. The proposed solution has been tested on real-world measured mmWave sensing and communication data, and the results show that it can achieve up to 98.19% accuracies while predicting top-13 beams. Correspondingly, when compared to existing been sweeping approach, the beam sweeping searching space and time overheads are greatly shortened roughly by 79.67% and 91.89%, respectively which confirm a promising solution for beamforming in mmWave enabled communications.", "field": ["Networking and Internet Architecture", "Artificial Intelligence", "Emerging Technologies", "Machine Learning", "Signal Processing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjE3Mw"}, {"title": "Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation", "authors": ["Chuanqi Cheng"], "date": "April 2025", "doi": "arXiv:2504.02438", "url": "https://arxiv.org/abs/2504.02438", "pdf": "https://arxiv.org/pdf/2504.02438", "abstract": "Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLaMP, a hierarchical video-language model that processes hour-long videos at ``mixed precision'' through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLaMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLaMP's superior performance across four video understanding benchmarks, particularly on long-form content. Notably, ViLaMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjQzOA"}, {"title": "Boundary representation learning via Transformer", "authors": ["Qiang Zou"], "date": "April 2025", "doi": "arXiv:2504.07134", "url": "https://arxiv.org/abs/2504.07134", "pdf": "https://arxiv.org/pdf/2504.07134", "abstract": "The recent rise of generative artificial intelligence (AI), powered by Transformer networks, has achieved remarkable success in natural language processing, computer vision, and graphics. However, the application of Transformers in computer-aided design (CAD), particularly for processing boundary representation (B-rep) models, remains largely unexplored. To bridge this gap, this paper introduces Boundary Representation Transformer (BRT), a novel method adapting Transformer for B-rep learning. B-rep models pose unique challenges due to their irregular topology and continuous geometric definitions, which are fundamentally different from the structured and discrete data Transformers are designed for. To address this, BRT proposes a continuous geometric embedding method that encodes B-rep surfaces (trimmed and untrimmed) into B\u00e9zier triangles, preserving their shape and continuity without discretization. Additionally, BRT employs a topology-aware embedding method that organizes these geometric embeddings into a sequence of discrete tokens suitable for Transformers, capturing both geometric and topological characteristics within B-rep models. This enables the Transformer's attention mechanism to effectively learn shape patterns and contextual semantics of boundary elements in a B-rep model. Extensive experiments demonstrate that BRT achieves state-of-the-art performance in part classification and feature recognition tasks.", "field": ["Graphics", "Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzEzNA"}, {"title": "OSCAR: Online Soft Compression And Reranking", "authors": ["Maxime Louis"], "date": "April 2025", "doi": "arXiv:2504.07109", "url": "https://arxiv.org/abs/2504.07109", "pdf": "https://arxiv.org/pdf/2504.07109", "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge, leading to improved accuracy and relevance. However, scaling RAG pipelines remains computationally expensive as retrieval sizes grow. To address this, we introduce OSCAR, a novel query-dependent online soft compression method that reduces computational overhead while preserving performance. Unlike traditional hard compression methods, which shorten retrieved texts, or soft compression approaches, which map documents to continuous embeddings offline, OSCAR dynamically compresses retrieved information at inference time, eliminating storage overhead and enabling higher compression rates. Additionally, we extend OSCAR to simultaneously perform reranking, further optimizing the efficiency of the RAG pipeline. Our experiments demonstrate state-of-the-art performance with a 2-5x speed-up in inference and minimal to no loss in accuracy for LLMs ranging from 1B to 24B parameters. The models are available at: https://huggingface.co/collections/naver/oscar-67d446a8e3a2551f57464295.", "field": ["Information Retrieval", "Artificial Intelligence", "Computation and Language"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzEwOQ"}, {"title": "Learning-Based Approximate Nonlinear Model Predictive Control Motion Cueing", "authors": ["Camilo Gonzalez Arango"], "date": "April 2025", "doi": "arXiv:2504.00469", "url": "https://arxiv.org/abs/2504.00469", "pdf": "https://arxiv.org/pdf/2504.00469", "abstract": "Motion Cueing Algorithms (MCAs) encode the movement of simulated vehicles into movement that can be reproduced with a motion simulator to provide a realistic driving experience within the capabilities of the machine. This paper introduces a novel learning-based MCA for serial robot-based motion simulators. Building on the differentiable predictive control framework, the proposed method merges the advantages of Nonlinear Model Predictive Control (NMPC) - notably nonlinear constraint handling and accurate kinematic modeling - with the computational efficiency of machine learning. By shifting the computational burden to offline training, the new algorithm enables real-time operation at high control rates, thus overcoming the key challenge associated with NMPC-based motion cueing. The proposed MCA incorporates a nonlinear joint-space plant model and a policy network trained to mimic NMPC behavior while accounting for joint acceleration, velocity, and position limits. Simulation experiments across multiple motion cueing scenarios showed that the proposed algorithm performed on par with a state-of-the-art NMPC-based alternative in terms of motion cueing quality as quantified by the RMSE and correlation coefficient with respect to reference signals. However, the proposed algorithm was on average 400 times faster than the NMPC baseline. In addition, the algorithm successfully generalized to unseen operating conditions, including motion cueing scenarios on a different vehicle and real-time physics-based simulations.", "field": ["Robotics", "Artificial Intelligence", "Systems and Control"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDQ2OQ"}, {"title": "Conthereum: Concurrent Ethereum Optimized Transaction Scheduling for Multi-Core Execution", "authors": ["Atefeh Zareh Chahoki"], "date": "April 2025", "doi": "arXiv:2504.07280", "url": "https://arxiv.org/abs/2504.07280", "pdf": "https://arxiv.org/pdf/2504.07280", "abstract": "Blockchain technology has revolutionized decentralized computation, providing high security through transparent cryptographic protocols and immutable data. However, the Blockchain Trilemma-an inherent trade-off between security, scalability, and performance-limits computational efficiency, resulting in low transactions-per-second (TPS) compared to conventional systems like Visa or PayPal. To address this, we introduce Conthereum, a novel concurrent blockchain solution that enhances multi-core usage in transaction processing through a deterministic scheduling scheme. It reformulates smart contract execution as a variant of the Flexible Job Shop Scheduling Problem (FJSS), optimizing both time and power consumption. Conthereum offers the most efficient open-source implementation compared to existing solutions. Empirical evaluations based on Ethereum, the most widely used blockchain platform, show near-linear throughput increases with available computational power. Additionally, an integrated energy consumption model allows participant to optimize power usage by intelligently distributing workloads across cores. This solution not only boosts network TPS and energy efficiency, offering a scalable and sustainable framework for blockchain transaction processing. The proposed approach also opens new avenues for further optimizations in Ethereum and is adaptable for broader applications in other blockchain infrastructures.", "field": ["Cryptography and Security", "Distributed, Parallel, and Cluster Computing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzI4MA"}, {"title": "STF-GCN: A Multi-Domain Graph Convolution Network Method for Automatic Modulation Recognition via Adaptive Correlation", "authors": ["Mingyuan Shao"], "date": "April 2025", "doi": "arXiv:2504.08504", "url": "https://arxiv.org/abs/2504.08504", "pdf": "https://arxiv.org/pdf/2504.08504", "abstract": "Automatic Modulation Recognition (AMR) is an essential part of Intelligent Transportation System (ITS) dynamic spectrum allocation. However, current deep learning-based AMR (DL-AMR) methods are challenged to extract discriminative and robust features at low signal-to-noise ratios (SNRs), where the representation of modulation symbols is highly interfered by noise. Furthermore, current research on GNN methods for AMR tasks generally suffers from issues related to graph structure construction and computational complexity. In this paper, we propose a Spatial-Temporal-Frequency Graph Convolution Network (STF-GCN) framework, with the temporal domain as the anchor point, to fuse spatial and frequency domain features embedded in the graph structure nodes. On this basis, an adaptive correlation-based adjacency matrix construction method is proposed, which significantly enhances the graph structure's capacity to aggregate local information into individual nodes. In addition, a PoolGAT layer is proposed to coarsen and compress the global key features of the graph, significantly reducing the computational complexity. The results of the experiments confirm that STF-GCN is able to achieve recognition performance far beyond the state-of-the-art DL-AMR algorithms, with overall accuracies of 64.35%, 66.04% and 70.95% on the RML2016.10a, RML2016.10b and RML22 datasets, respectively. Furthermore, the average recognition accuracies under low SNR conditions from -14dB to 0dB outperform the state-of-the-art (SOTA) models by 1.20%, 1.95% and 1.83%, respectively.", "field": ["Signal Processing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wODUwNA"}, {"title": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language Generation", "authors": ["Vishal Gandhi"], "date": "April 2025", "doi": "arXiv:2504.08281", "url": "https://arxiv.org/abs/2504.08281", "pdf": "https://arxiv.org/pdf/2504.08281", "abstract": "Advancements in emotion aware language processing increasingly shape vital NLP applications ranging from conversational AI and affective computing to computational psychology and creative content generation. Existing emotion datasets either lack emotional granularity or fail to capture necessary stylistic diversity, limiting the advancement of effective emotion conditioned text generation systems. Seeking to bridge this crucial gap between granularity and style diversity, this paper introduces a novel systematically constructed dataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine grained emotion taxonomies adapted from existing sources such as dair ai emotion dataset and GoEmotions taxonomy. This dataset comprises multiple emotionally nuanced variations of original sentences regenerated across distinct contextual styles such as conversational, formal, poetic, and narrative, using advanced Large Language Models LLMs. Rigorous computational evaluation using metrics such as perplexity, embedding variance, readability, lexical diversity, and semantic coherence measures validates the datasets emotional authenticity, linguistic fluency, and textual diversity. Comprehensive metric analyses affirm its potential to support deeper explorations into emotion conditioned style adaptive text generation. By enabling precision tuned emotionally nuanced language modeling, our dataset creates fertile ground for research on fine grained emotional control, prompt driven explanation, interpretability, and style adaptive expressive language generation with LLMs.", "field": ["Computation and Language", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wODI4MQ"}, {"title": "Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group Transformations", "authors": ["Chongjie Si"], "date": "April 2025", "doi": "arXiv:2504.00851", "url": "https://arxiv.org/abs/2504.00851", "pdf": "https://arxiv.org/pdf/2504.00851", "abstract": "Adapting pre-trained foundation models for diverse downstream tasks is a core practice in artificial intelligence. However, the wide range of tasks and high computational costs make full fine-tuning impractical. To overcome this, parameter-efficient fine-tuning (PEFT) methods like LoRA have emerged and are becoming a growing research focus. Despite the success of these methods, they are primarily designed for linear layers, focusing on two-dimensional matrices while largely ignoring higher-dimensional parameter spaces like convolutional kernels. Moreover, directly applying these methods to higher-dimensional parameter spaces often disrupts their structural relationships. Given the rapid advancements in matrix-based PEFT methods, rather than designing a specialized strategy, we propose a generalization that extends matrix-based PEFT methods to higher-dimensional parameter spaces without compromising their structural properties. Specifically, we treat parameters as elements of a Lie group, with updates modeled as perturbations in the corresponding Lie algebra. These perturbations are mapped back to the Lie group through the exponential map, ensuring smooth, consistent updates that preserve the inherent structure of the parameter space. Extensive experiments on computer vision and natural language processing validate the effectiveness and versatility of our approach, demonstrating clear improvements over existing methods.", "field": ["Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDg1MQ"}, {"title": "Task-Aware Parameter-Efficient Fine-Tuning of Large Pre-Trained Models at the Edge", "authors": ["Senkang Hu"], "date": "April 2025", "doi": "arXiv:2504.03718", "url": "https://arxiv.org/abs/2504.03718", "pdf": "https://arxiv.org/pdf/2504.03718", "abstract": "Large language models (LLMs) have achieved remarkable success in various tasks, such as decision-making, reasoning, and question answering. They have been widely used in edge devices. However, fine-tuning LLMs to specific tasks at the edge is challenging due to the high computational cost and the limited storage and energy resources at the edge. To address this issue, we propose TaskEdge, a task-aware parameter-efficient fine-tuning framework at the edge, which allocates the most effective parameters to the target task and only updates the task-specific parameters. Specifically, we first design a parameter importance calculation criterion that incorporates both weights and input activations into the computation of weight importance. Then, we propose a model-agnostic task-specific parameter allocation algorithm to ensure that task-specific parameters are distributed evenly across the model, rather than being concentrated in specific regions. In doing so, TaskEdge can significantly reduce the computational cost and memory usage while maintaining performance on the target downstream tasks by updating less than 0.1\\% of the parameters. In addition, TaskEdge can be easily integrated with structured sparsity to enable acceleration by NVIDIA's specialized sparse tensor cores, and it can be seamlessly integrated with LoRA to enable efficient sparse low-rank adaptation. Extensive experiments on various tasks demonstrate the effectiveness of TaskEdge.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzcxOA"}, {"title": "Falafels: A tool for Estimating Federated Learning Energy Consumption via Discrete Simulation", "authors": ["Andrew Mary Huet de Barochez"], "date": "April 2025", "doi": "arXiv:2504.03660", "url": "https://arxiv.org/abs/2504.03660", "pdf": "https://arxiv.org/pdf/2504.03660", "abstract": "The growth in computational power and data hungriness of Machine Learning has led to an important shift of research efforts towards the distribution of ML models on multiple machines, leading in even more powerful models. However, there exists many Distributed Artificial Intelligence paradigms and for each of them the platform and algorithm configurations play an important role in terms of training time and energy consumption. Many mathematical models and frameworks can respectively predict and benchmark this energy consumption, nonetheless, the former lacks of realism and extensibility while the latter suffers high run-times and actual power consumption. In this article, we introduce Falafels, an extensible tool that predicts the energy consumption and training time of -but not limited to -Federated Learning systems. It distinguishes itself with its discrete-simulatorbased solution leading to nearly instant run-time and fast development of new algorithms. Furthermore, we show this approach permits the use of an evolutionary algorithm providing the ability to optimize the system configuration for a given machine learning workload.", "field": ["Distributed, Parallel, and Cluster Computing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzY2MA"}, {"title": "Resource-Efficient Beam Prediction in mmWave Communications with Multimodal Realistic Simulation Framework", "authors": ["Yu Min Park"], "date": "April 2025", "doi": "arXiv:2504.05187", "url": "https://arxiv.org/abs/2504.05187", "pdf": "https://arxiv.org/pdf/2504.05187", "abstract": "Beamforming is a key technology in millimeter-wave (mmWave) communications that improves signal transmission by optimizing directionality and intensity. However, conventional channel estimation methods, such as pilot signals or beam sweeping, often fail to adapt to rapidly changing communication environments. To address this limitation, multimodal sensing-aided beam prediction has gained significant attention, using various sensing data from devices such as LiDAR, radar, GPS, and RGB images to predict user locations or network conditions. Despite its promising potential, the adoption of multimodal sensing-aided beam prediction is hindered by high computational complexity, high costs, and limited datasets. Thus, in this paper, a resource-efficient learning approach is proposed to transfer knowledge from a multimodal network to a monomodal (radar-only) network based on cross-modal relational knowledge distillation (CRKD), while reducing computational overhead and preserving predictive accuracy. To enable multimodal learning with realistic data, a novel multimodal simulation framework is developed while integrating sensor data generated from the autonomous driving simulator CARLA with MATLAB-based mmWave channel modeling, and reflecting real-world conditions. The proposed CRKD achieves its objective by distilling relational information across different feature spaces, which enhances beam prediction performance without relying on expensive sensor data. Simulation results demonstrate that CRKD efficiently distills multimodal knowledge, allowing a radar-only model to achieve $94.62\\%$ of the teacher performance. In particular, this is achieved with just $10\\%$ of the teacher network's parameters, thereby significantly reducing computational complexity and dependence on multimodal sensor data.", "field": ["Networking and Internet Architecture", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTE4Nw"}, {"title": "Planning Safety Trajectories with Dual-Phase, Physics-Informed, and Transportation Knowledge-Driven Large Language Models", "authors": ["Rui Gan"], "date": "April 2025", "doi": "arXiv:2504.04562", "url": "https://arxiv.org/abs/2504.04562", "pdf": "https://arxiv.org/pdf/2504.04562", "abstract": "Foundation models have demonstrated strong reasoning and generalization capabilities in driving-related tasks, including scene understanding, planning, and control. However, they still face challenges in hallucinations, uncertainty, and long inference latency. While existing foundation models have general knowledge of avoiding collisions, they often lack transportation-specific safety knowledge. To overcome these limitations, we introduce LetsPi, a physics-informed, dual-phase, knowledge-driven framework for safe, human-like trajectory planning. To prevent hallucinations and minimize uncertainty, this hybrid framework integrates Large Language Model (LLM) reasoning with physics-informed social force dynamics. LetsPi leverages the LLM to analyze driving scenes and historical information, providing appropriate parameters and target destinations (goals) for the social force model, which then generates the future trajectory. Moreover, the dual-phase architecture balances reasoning and computational efficiency through its Memory Collection phase and Fast Inference phase. The Memory Collection phase leverages the physics-informed LLM to process and refine planning results through reasoning, reflection, and memory modules, storing safe, high-quality driving experiences in a memory bank. Surrogate safety measures and physics-informed prompt techniques are introduced to enhance the LLM's knowledge of transportation safety and physical force, respectively. The Fast Inference phase extracts similar driving experiences as few-shot examples for new scenarios, while simplifying input-output requirements to enable rapid trajectory planning without compromising safety. Extensive experiments using the HighD dataset demonstrate that LetsPi outperforms baseline models across five safety metrics.See PDF for project Github link.", "field": ["Robotics", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDU2Mg"}, {"title": "The Dream Within Huang Long Cave: AI-Driven Interactive Narrative for Family Storytelling and Emotional Reflection", "authors": ["Jiayang Huang"], "date": "April 2025", "doi": "arXiv:2504.04968", "url": "https://arxiv.org/abs/2504.04968", "pdf": "https://arxiv.org/pdf/2504.04968", "abstract": "This paper introduces the art project The Dream Within Huang Long Cave, an AI-driven interactive and immersive narrative experience. The project offers new insights into AI technology, artistic practice, and psychoanalysis. Inspired by actual geographical landscapes and familial archetypes, the work combines psychoanalytic theory and computational technology, providing an artistic response to the concept of the non-existence of the Big Other. The narrative is driven by a combination of a large language model (LLM) and a realistic digital character, forming a virtual agent named YELL. Through dialogue and exploration within a cave automatic virtual environment (CAVE), the audience is invited to unravel the language puzzles presented by YELL and help him overcome his life challenges. YELL is a fictional embodiment of the Big Other, modeled after the artist's real father. Through a cross-temporal interaction with this digital father, the project seeks to deconstruct complex familial relationships. By demonstrating the non-existence of the Big Other, we aim to underscore the authenticity of interpersonal emotions, positioning art as a bridge for emotional connection and understanding within family dynamics.", "field": ["Multimedia", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDk2OA"}, {"title": "A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam", "authors": ["Rean Fernandes"], "date": "April 2025", "doi": "arXiv:2504.04945", "url": "https://arxiv.org/abs/2504.04945", "pdf": "https://arxiv.org/pdf/2504.04945", "abstract": "Legal reasoning tasks present unique challenges for large language models (LLMs) due to the complexity of domain-specific knowledge and reasoning processes. This paper investigates how effectively smaller language models (Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514 Multi-state Bar Examination (MBE) questions to improve legal question answering accuracy. We evaluate these models on the 2022 MBE questions licensed from JD Advising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our methodology involves collecting approximately 200 questions per legal domain across 7 domains. We distill the dataset using Llama 3 (70B) to transform explanations into a structured IRAC (Issue, Rule, Application, Conclusion) format as a guided reasoning process to see if it results in better performance over the non-distilled dataset. We compare the non-fine-tuned models against their supervised fine-tuned (SFT) counterparts, trained for different sample sizes per domain, to study the effect on accuracy and prompt adherence. We also analyse option selection biases and their mitigation following SFT. In addition, we consolidate the performance across multiple variables: prompt type (few-shot vs zero-shot), answer ordering (chosen-option first vs generated-explanation first), response format (Numbered list vs Markdown vs JSON), and different decoding temperatures. Our findings show that domain-specific SFT helps some model configurations achieve close to human baseline performance, despite limited computational resources and a relatively small dataset. We release both the gathered SFT dataset and the family of Supervised Fine-tuned (SFT) adapters optimised for MBE performance. This establishes a practical lower bound on resources needed towards achieving effective legal question answering in smaller LLMs.", "field": ["Machine Learning", "Artificial Intelligence", "Computation and Language"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDk0NQ"}, {"title": "Cloud-Fog Automation: The New Paradigm towards Autonomous Industrial Cyber-Physical Systems", "authors": ["Jiong Jin"], "date": "April 2025", "doi": "arXiv:2504.04908", "url": "https://arxiv.org/abs/2504.04908", "pdf": "https://arxiv.org/pdf/2504.04908", "abstract": "Autonomous Industrial Cyber-Physical Systems (ICPS) represent a future vision where industrial systems achieve full autonomy, integrating physical processes seamlessly with communication, computing and control technologies while holistically embedding intelligence. Cloud-Fog Automation is a new digitalized industrial automation reference architecture that has been recently proposed. This architecture is a fundamental paradigm shift from the traditional International Society of Automation (ISA)-95 model to accelerate the convergence and synergy of communication, computing, and control towards a fully autonomous ICPS. With the deployment of new wireless technologies to enable almost-deterministic ultra-reliable low-latency communications, a joint design of optimal control and computing has become increasingly important in modern ICPS. It is also imperative that system-wide cyber-physical security are critically enforced. Despite recent advancements in the field, there are still significant research gaps and open technical challenges. Therefore, a deliberate rethink in co-designing and synergizing communications, computing, and control (which we term \"3C co-design\") is required. In this paper, we position Cloud-Fog Automation with 3C co-design as the new paradigm to realize the vision of autonomous ICPS. We articulate the state-of-the-art and future directions in the field, and specifically discuss how goal-oriented communication, virtualization-empowered computing, and Quality of Service (QoS)-aware control can drive Cloud-Fog Automation towards a fully autonomous ICPS, while accounting for system-wide cyber-physical security.", "field": ["Systems and Control"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDkwOA"}, {"title": "F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group Relative Policy Optimization", "authors": ["Xiaohui Sun"], "date": "April 2025", "doi": "arXiv:2504.02407", "url": "https://arxiv.org/abs/2504.02407", "pdf": "https://arxiv.org/pdf/2504.02407", "abstract": "We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Gradient Reward Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (a 29.5% relative reduction in WER) and speaker similarity (a 4.6% relative increase in SIM score) compared to conventional flow-matching based TTS systems. Audio samples are available at https://frontierlabs.github.io/F5R.", "field": ["Sound", "Audio and Speech Processing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjQwNw"}, {"title": "InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction Features", "authors": ["Sujay Khandagale"], "date": "April 2025", "doi": "arXiv:2504.06609", "url": "https://arxiv.org/abs/2504.06609", "pdf": "https://arxiv.org/pdf/2504.06609", "abstract": "Modern search systems use a multi-stage architecture to deliver personalized results efficiently. Key stages include retrieval, pre-ranking, full ranking, and blending, which refine billions of items to top selections. The pre-ranking stage, vital for scoring and filtering hundreds of thousands of items down to a few thousand, typically relies on two tower models due to their computational efficiency, despite often lacking in capturing complex interactions. While query-item cross interaction features are paramount for full ranking, integrating them into pre-ranking models presents efficiency-related challenges. In this paper, we introduce InteractRank, a novel two tower pre-ranking model with robust cross interaction features used at Pinterest. By incorporating historical user engagement-based query-item interactions in the scoring function along with the two tower dot product, InteractRank significantly boosts pre-ranking performance with minimal latency and computation costs. In real-world A/B experiments at Pinterest, InteractRank improves the online engagement metric by 6.5% over a BM25 baseline and by 3.7% over a vanilla two tower baseline. We also highlight other components of InteractRank, like real-time user-sequence modeling, and analyze their contributions through offline ablation studies. The code for InteractRank is available at https://github.com/pinterest/atg-research/tree/main/InteractRank.", "field": ["Information Retrieval", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjYwOQ"}, {"title": "Entropy-Based Block Pruning for Efficient Large Language Models", "authors": ["Liangwei Yang"], "date": "April 2025", "doi": "arXiv:2504.03794", "url": "https://arxiv.org/abs/2504.03794", "pdf": "https://arxiv.org/pdf/2504.03794", "abstract": "As large language models continue to scale, their growing computational and storage demands pose significant challenges for real-world deployment. In this work, we investigate redundancy within Transformer-based models and propose an entropy-based pruning strategy to enhance efficiency while maintaining performance. Empirical analysis reveals that the entropy of hidden representations decreases in the early blocks but progressively increases across most subsequent blocks. This trend suggests that entropy serves as a more effective measure of information richness within computation blocks. Unlike cosine similarity, which primarily captures geometric relationships, entropy directly quantifies uncertainty and information content, making it a more reliable criterion for pruning. Extensive experiments demonstrate that our entropy-based pruning approach surpasses cosine similarity-based methods in reducing model size while preserving accuracy, offering a promising direction for efficient model deployment.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzc5NA"}, {"title": "A Survey of Pathology Foundation Model: Progress and Future Directions", "authors": ["Conghao Xiong"], "date": "April 2025", "doi": "arXiv:2504.04045", "url": "https://arxiv.org/abs/2504.04045", "pdf": "https://arxiv.org/pdf/2504.04045", "abstract": "Computational pathology, analyzing whole slide images for automated cancer diagnosis, relies on the multiple instance learning framework where performance heavily depends on the feature extractor and aggregator. Recent Pathology Foundation Models (PFMs), pretrained on large-scale histopathology data, have significantly enhanced capabilities of extractors and aggregators but lack systematic analysis frameworks. This survey presents a hierarchical taxonomy organizing PFMs through a top-down philosophy that can be utilized to analyze FMs in any domain: model scope, model pretraining, and model design. Additionally, we systematically categorize PFM evaluation tasks into slide-level, patch-level, multimodal, and biological tasks, providing comprehensive benchmarking criteria. Our analysis identifies critical challenges in both PFM development (pathology-specific methodology, end-to-end pretraining, data-model scalability) and utilization (effective adaptation, model maintenance), paving the way for future directions in this promising field. Resources referenced in this survey are available at https://github.com/BearCleverProud/AwesomeWSI.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDA0NQ"}, {"title": "Gating is Weighting: Understanding Gated Linear Attention through In-context Learning", "authors": ["Yingcong Li"], "date": "April 2025", "doi": "arXiv:2504.04308", "url": "https://arxiv.org/abs/2504.04308", "pdf": "https://arxiv.org/pdf/2504.04308", "abstract": "Linear attention methods offer a compelling alternative to softmax attention due to their efficiency in recurrent decoding. Recent research has focused on enhancing standard linear attention by incorporating gating while retaining its computational benefits. Such Gated Linear Attention (GLA) architectures include competitive models such as Mamba and RWKV. In this work, we investigate the in-context learning capabilities of the GLA model and make the following contributions. We show that a multilayer GLA can implement a general class of Weighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent weights. These weights are induced by the gating mechanism and the input, enabling the model to control the contribution of individual tokens to prediction. To further understand the mechanics of this weighting, we introduce a novel data model with multitask prompts and characterize the optimization landscape of learning a WPGD algorithm. Under mild conditions, we establish the existence and uniqueness (up to scaling) of a global minimum, corresponding to a unique WPGD solution. Finally, we translate these findings to explore the optimization landscape of GLA and shed light on how gating facilitates context-aware learning and when it is provably better than vanilla linear attention.", "field": ["Machine Learning", "Artificial Intelligence", "Computation and Language", "Optimization and Control"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNDMwOA"}, {"title": "Watts-Per-Intelligence: Part I (Energy Efficiency)", "authors": ["Elija Perrier"], "date": "April 2025", "doi": "arXiv:2504.05328", "url": "https://arxiv.org/abs/2504.05328", "pdf": "https://arxiv.org/pdf/2504.05328", "abstract": "We present a mathematical framework for quantifying energy efficiency in intelligent systems by linking energy consumption to information processing capacity. Our proposed watts per intelligence metric integrates algorithmic thermodynamic principles of Landauer with computational models of machine intelligence. By formalising the irreversible energy costs of computation, we derive rigorous lower bounds on energy usage of algorithmic intelligent systems and their adaptability. We introduce theorems that constrain the trade offs between intelligence output and energy expenditure. Our results contribute to design principles for energy efficient intelligent systems.", "field": ["Information Theory"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTMyOA"}, {"title": "Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning", "authors": ["Haoran Zhang"], "date": "April 2025", "doi": "arXiv:2504.05138", "url": "https://arxiv.org/abs/2504.05138", "pdf": "https://arxiv.org/pdf/2504.05138", "abstract": "Federated learning (FL) allows edge devices to collaboratively train models without sharing local data. As FL gains popularity, clients may need to train multiple unrelated FL models, but communication constraints limit their ability to train all models simultaneously. While clients could train FL models sequentially, opportunistically having FL clients concurrently train different models -- termed multi-model federated learning (MMFL) -- can reduce the overall training time. Prior work uses simple client-to-model assignments that do not optimize the contribution of each client to each model over the course of its training. Prior work on single-model FL shows that intelligent client selection can greatly accelerate convergence, but na\u00efve extensions to MMFL can violate heterogeneous resource constraints at both the server and the clients. In this work, we develop a novel convergence analysis of MMFL with arbitrary client sampling methods, theoretically demonstrating the strengths and limitations of previous well-established gradient-based methods. Motivated by this analysis, we propose MMFL-LVR, a loss-based sampling method that minimizes training variance while explicitly respecting communication limits at the server and reducing computational costs at the clients. We extend this to MMFL-StaleVR, which incorporates stale updates for improved efficiency and stability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead deployment. Experiments show our methods improve average accuracy by up to 19.1% over random sampling, with only a 5.4% gap from the theoretical optimum (full client participation).", "field": ["Machine Learning", "Distributed, Parallel, and Cluster Computing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTEzOA"}, {"title": "Are Generative AI Agents Effective Personalized Financial Advisors?", "authors": ["Takehiro Takayanagi"], "date": "April 2025", "doi": "arXiv:2504.05862", "url": "https://arxiv.org/abs/2504.05862", "pdf": "https://arxiv.org/pdf/2504.05862", "abstract": "Large language model-based agents are becoming increasingly popular as a low-cost mechanism to provide personalized, conversational advice, and have demonstrated impressive capabilities in relatively simple scenarios, such as movie recommendations. But how do these agents perform in complex high-stakes domains, where domain expertise is essential and mistakes carry substantial risk? This paper investigates the effectiveness of LLM-advisors in the finance domain, focusing on three distinct challenges: (1) eliciting user preferences when users themselves may be unsure of their needs, (2) providing personalized guidance for diverse investment preferences, and (3) leveraging advisor personality to build relationships and foster trust. Via a lab-based user study with 64 participants, we show that LLM-advisors often match human advisor performance when eliciting preferences, although they can struggle to resolve conflicting user needs. When providing personalized advice, the LLM was able to positively influence user behavior, but demonstrated clear failure modes. Our results show that accurate preference elicitation is key, otherwise, the LLM-advisor has little impact, or can even direct the investor toward unsuitable assets. More worryingly, users appear insensitive to the quality of advice being given, or worse these can have an inverse relationship. Indeed, users reported a preference for and increased satisfaction as well as emotional trust with LLMs adopting an extroverted persona, even though those agents provided worse advice.", "field": ["Artificial Intelligence", "Computation and Language", "Human-Computer Interaction", "Information Retrieval", "Computational Finance"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTg2Mg"}, {"title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM", "authors": ["Jirong Zha"], "date": "April 2025", "doi": "arXiv:2504.05786", "url": "https://arxiv.org/abs/2504.05786", "pdf": "https://arxiv.org/pdf/2504.05786", "abstract": "3D spatial understanding is essential in real-world applications such as robotics, autonomous vehicles, virtual reality, and medical imaging. Recently, Large Language Models (LLMs), having demonstrated remarkable success across various domains, have been leveraged to enhance 3D understanding tasks, showing potential to surpass traditional computer vision methods. In this survey, we present a comprehensive review of methods integrating LLMs with 3D spatial understanding. We propose a taxonomy that categorizes existing methods into three branches: image-based methods deriving 3D understanding from 2D visual data, point cloud-based methods working directly with 3D representations, and hybrid modality-based methods combining multiple data streams. We systematically review representative methods along these categories, covering data representations, architectural modifications, and training strategies that bridge textual and 3D modalities. Finally, we discuss current limitations, including dataset scarcity and computational challenges, while highlighting promising research directions in spatial perception, multi-modal fusion, and real-world applications.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTc4Ng"}, {"title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement", "authors": ["Xingshan Zeng"], "date": "April 2025", "doi": "arXiv:2504.01400", "url": "https://arxiv.org/abs/2504.01400", "pdf": "https://arxiv.org/pdf/2504.01400", "abstract": "Tool learning, which allows Large Language Models (LLMs) to leverage external tools for solving complex user tasks, has emerged as a promising avenue for extending model capabilities. However, current approaches primarily focus on data synthesis for fine-tuning LLMs to invoke tools effectively, largely ignoring how to fully stimulate the potential of the model. In this paper, we propose ToolACE-R, a novel method that introduces adaptive self-refinement for tool invocations. Our approach features a model-aware iterative training procedure that progressively incorporates more training samples based on the model's evolving capabilities. Additionally, it allows LLMs to iteratively refine their tool calls, optimizing performance without requiring external feedback. To further enhance computational efficiency, we integrate an adaptive mechanism when scaling the inference time, enabling the model to autonomously determine when to stop the refinement process. We conduct extensive experiments across several benchmark datasets, showing that ToolACE-R achieves competitive performance compared to advanced API-based models, even without any refinement. Furthermore, its performance can be further improved efficiently through adaptive self-refinement. Our results demonstrate the effectiveness of the proposed method, which is compatible with base models of various sizes, offering a promising direction for more efficient tool learning.", "field": ["Computation and Language", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTQwMA"}, {"title": "AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization", "authors": ["Chaohu Liu"], "date": "April 2025", "doi": "arXiv:2504.01735", "url": "https://arxiv.org/abs/2504.01735", "pdf": "https://arxiv.org/pdf/2504.01735", "abstract": "Large Vision-Language Models (LVLMs), such as GPT-4o and LLaVA, have recently witnessed remarkable advancements and are increasingly being deployed in real-world applications. However, inheriting the sensitivity of visual neural networks, LVLMs remain vulnerable to adversarial attacks, which can result in erroneous or malicious outputs. While existing efforts utilize adversarial fine-tuning to enhance robustness, they often suffer from performance degradation on clean inputs. In this paper, we proposes AdPO, a novel adversarial defense strategy for LVLMs based on preference optimization. For the first time, we reframe adversarial training as a preference optimization problem, aiming to enhance the model's preference for generating normal outputs on clean inputs while rejecting the potential misleading outputs for adversarial examples. Notably, AdPO achieves this by solely modifying the image encoder, e.g., CLIP ViT, resulting in superior clean and adversarial performance in a variety of downsream tasks. Considering that training involves large language models (LLMs), the computational cost increases significantly. We validate that training on smaller LVLMs and subsequently transferring to larger models can achieve competitive performance while maintaining efficiency comparable to baseline methods. Our comprehensive experiments confirm the effectiveness of the proposed AdPO, which provides a novel perspective for future adversarial defense research.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTczNQ"}, {"title": "Vers une mod\u00e9lisation de la confiance dans le renseignement sur les menaces cyber", "authors": ["Laurent Bobelin"], "date": "April 2025", "doi": "arXiv:2504.01606", "url": "https://arxiv.org/abs/2504.01606", "pdf": "https://arxiv.org/pdf/2504.01606", "abstract": "Cyber threat intelligence (CTI) is essential for effective system defense. CTI is a collection of information about current or past threats to a computer system. This information is gathered by an agent through observation, or based on a set of sources. Building intelligence only makes sense if you have confidence in it. To achieve this, it is necessary to estimate the confidence in each piece of information gathered, taking into account the different dimensions that can make it up: reliability of the source, competence, plausibility of the information, credibility of the information, for example. The information gathered must then be combined with other information to consolidate an agent's knowledge. Recent advances have been made in the theory underlying the modeling of trust for decision-making based on uncertain information, notably by using multivalued logic. This approach makes it possible to deal with unknown values of trust-building parameters, or to easily integrate dimensions. In this article we present the problem of CTI and CTI information sharing, and the reasons that led us to use a logic-based solution for an initial implementation.", "field": ["Cryptography and Security", "Logic in Computer Science"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTYwNg"}, {"title": "GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution Kernel Using Gaussian Mixture Rings", "authors": ["Yuexi Du"], "date": "April 2025", "doi": "arXiv:2504.02819", "url": "https://arxiv.org/abs/2504.02819", "pdf": "https://arxiv.org/pdf/2504.02819", "abstract": "Symmetry, where certain features remain invariant under geometric transformations, can often serve as a powerful prior in designing convolutional neural networks (CNNs). While conventional CNNs inherently support translational equivariance, extending this property to rotation and reflection has proven challenging, often forcing a compromise between equivariance, efficiency, and information loss. In this work, we introduce Gaussian Mixture Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths radial symmetry using a mixture of Gaussian-weighted rings. This design mitigates discretization errors of circular kernels, thereby preserving robust rotation and reflection equivariance without incurring computational overhead. We further optimize both the space and speed efficiency of GMR-Conv via a novel parameterization and computation strategy, allowing larger kernels at an acceptable cost. Extensive experiments on eight classification and one segmentation datasets demonstrate that GMR-Conv not only matches conventional CNNs' performance but can also surpass it in applications with orientation-less data. GMR-Conv is also proven to be more robust and efficient than the state-of-the-art equivariant learning methods. Our work provides inspiring empirical evidence that carefully applied radial symmetry can alleviate the challenges of information loss, marking a promising advance in equivariant network architectures. The code is available at https://github.com/XYPB/GMR-Conv.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence", "Image and Video Processing", "Signal Processing"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjgxOQ"}, {"title": "Differentiable Optimization for Deep Learning-Enhanced DC Approximation of AC Optimal Power Flow", "authors": ["Andrew Rosemberg"], "date": "April 2025", "doi": "arXiv:2504.01970", "url": "https://arxiv.org/abs/2504.01970", "pdf": "https://arxiv.org/pdf/2504.01970", "abstract": "The growing scale of power systems and the increasing uncertainty introduced by renewable energy sources necessitates novel optimization techniques that are significantly faster and more accurate than existing methods. The AC Optimal Power Flow (AC-OPF) problem, a core component of power grid optimization, is often approximated using linearized DC Optimal Power Flow (DC-OPF) models for computational tractability, albeit at the cost of suboptimal and inefficient decisions. To address these limitations, we propose a novel deep learning-based framework for network equivalency that enhances DC-OPF to more closely mimic the behavior of AC-OPF. The approach utilizes recent advances in differentiable optimization, incorporating a neural network trained to predict adjusted nodal shunt conductances and branch susceptances in order to account for nonlinear power flow behavior. The model can be trained end-to-end using modern deep learning frameworks by leveraging the implicit function theorem. Results demonstrate the framework's ability to significantly improve prediction accuracy, paving the way for more reliable and efficient power systems.", "field": ["Optimization and Control", "Artificial Intelligence", "Machine Learning", "Systems and Control"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMTk3MA"}, {"title": "MetaCLBench: Meta Continual Learning Benchmark on Resource-Constrained Edge Devices", "authors": ["Sijia Li"], "date": "April 2025", "doi": "arXiv:2504.00174", "url": "https://arxiv.org/abs/2504.00174", "pdf": "https://arxiv.org/pdf/2504.00174", "abstract": "Meta-Continual Learning (Meta-CL) has emerged as a promising approach to minimize manual labeling efforts and system resource requirements by enabling Continual Learning (CL) with limited labeled samples. However, while existing methods have shown success in image-based tasks, their effectiveness remains unexplored for sequential time-series data from sensor systems, particularly audio inputs. To address this gap, we conduct a comprehensive benchmark study evaluating six representative Meta-CL approaches using three network architectures on five datasets from both image and audio modalities. We develop MetaCLBench, an end-to-end Meta-CL benchmark framework for edge devices to evaluate system overheads and investigate trade-offs among performance, computational costs, and memory requirements across various Meta-CL methods. Our results reveal that while many Meta-CL methods enable to learn new classes for both image and audio modalities, they impose significant computational and memory costs on edge devices. Also, we find that pre-training and meta-training procedures based on source data before deployment improve Meta-CL performance. Finally, to facilitate further research, we provide practical guidelines for researchers and machine learning practitioners implementing Meta-CL on resource-constrained environments and make our benchmark framework and tools publicly available, enabling fair evaluation across both accuracy and system-level metrics.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDE3NA"}, {"title": "Distill-C: Enhanced NL2SQL via Distilled Customization with LLMs", "authors": ["Cong Duy Vu Hoang"], "date": "April 2025", "doi": "arXiv:2504.00048", "url": "https://arxiv.org/abs/2504.00048", "pdf": "https://arxiv.org/pdf/2504.00048", "abstract": "The growing adoption of large language models (LLMs) in business applications has amplified interest in Natural Language to SQL (NL2SQL) solutions, in which there is competing demand for high performance and efficiency. Domain- and customer-specific requirements further complicate the problem. To address this conundrum, we introduce Distill-C, a distilled customization framework tailored for NL2SQL tasks. Distill-C utilizes large teacher LLMs to produce high-quality synthetic data through a robust and scalable pipeline. Finetuning smaller and open-source LLMs on this synthesized data enables them to rival or outperform teacher models an order of magnitude larger. Evaluated on multiple challenging benchmarks, Distill-C achieves an average improvement of 36% in execution accuracy compared to the base models from three distinct LLM families. Additionally, on three internal customer benchmarks, Distill-C demonstrates a 22.6% performance improvement over the base models. Our results demonstrate that Distill-C is an effective, high-performing and generalizable approach for deploying lightweight yet powerful NL2SQL models, delivering exceptional accuracies while maintaining low computational cost.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDA0OA"}, {"title": "MetaLoRA: Tensor-Enhanced Adaptive Low-Rank Fine-tuning", "authors": ["Maolin Wang"], "date": "April 2025", "doi": "arXiv:2504.00460", "url": "https://arxiv.org/abs/2504.00460", "pdf": "https://arxiv.org/pdf/2504.00460", "abstract": "There has been a significant increase in the deployment of neural network models, presenting substantial challenges in model adaptation and fine-tuning. Efficient adaptation is crucial in maintaining model performance across diverse tasks and domains. While Low-Rank Adaptation (LoRA) has emerged as a promising parameter-efficient fine-tuning method, its fixed parameter nature limits its ability to handle dynamic task requirements effectively. Adapting models to new tasks can be challenging due to the need for extensive fine-tuning. Current LoRA variants primarily focus on general parameter reduction while overlooking the importance of dynamic parameter adjustment and meta-learning capabilities. Moreover, existing approaches mainly address static adaptations, neglecting the potential benefits of task-aware parameter generation in handling diverse task distributions. To address these limitations, this Ph.D. research proposes a LoRA generation approach to model task relationships and introduces MetaLoRA, a novel parameter-efficient adaptation framework incorporating meta-learning principles. This work develops a comprehensive architecture that integrates meta-parameter generation with adaptive low-rank decomposition, enabling efficient handling of both task-specific and task-agnostic features. MetaLoRA accurately captures task patterns by incorporating meta-learning mechanisms and dynamic parameter adjustment strategies. To our knowledge, this research represents the first attempt to provide a meta-learning enhanced LoRA variant, offering improved adaptation capability while maintaining computational efficiency in model fine-tuning.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDQ2MA"}, {"title": "Rack Position Optimization in Large-Scale Heterogeneous Data Centers", "authors": ["Chang-Lin Chen"], "date": "April 2025", "doi": "arXiv:2504.00277", "url": "https://arxiv.org/abs/2504.00277", "pdf": "https://arxiv.org/pdf/2504.00277", "abstract": "As rapidly growing AI computational demands accelerate the need for new hardware installation and maintenance, this work explores optimal data center resource management by balancing operational efficiency with fault tolerance through strategic rack positioning considering diverse resources and locations. Traditional mixed-integer programming (MIP) approaches often struggle with scalability, while heuristic methods may result in significant sub-optimality. To address these issues, this paper presents a novel two-tier optimization framework using a high-level deep reinforcement learning (DRL) model to guide a low-level gradient-based heuristic for local search. The high-level DRL agent employs Leader Reward for optimal rack type ordering, and the low-level heuristic efficiently maps racks to positions, minimizing movement counts and ensuring fault-tolerant resource distribution. This approach allows scalability to over 100,000 positions and 100 rack types. Our method outperformed the gradient-based heuristic by 7\\% on average and the MIP solver by over 30\\% in objective value. It achieved a 100\\% success rate versus MIP's 97.5\\% (within a 20-minute limit), completing in just 2 minutes compared to MIP's 1630 minutes (i.e., almost 4 orders of magnitude improvement). Unlike the MIP solver, which showed performance variability under time constraints and high penalties, our algorithm consistently delivered stable, efficient results - an essential feature for large-scale data center management.", "field": ["Artificial Intelligence", "Distributed, Parallel, and Cluster Computing", "Machine Learning", "Networking and Internet Architecture", "Optimization and Control"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDI3Nw"}, {"title": "PLM-eXplain: Divide and Conquer the Protein Embedding Space", "authors": ["Jan van Eck"], "date": "April 2025", "doi": "arXiv:2504.07156", "url": "https://arxiv.org/abs/2504.07156", "pdf": "https://arxiv.org/pdf/2504.07156", "abstract": "Protein language models (PLMs) have revolutionised computational biology through their ability to generate powerful sequence representations for diverse prediction tasks. However, their black-box nature limits biological interpretation and translation to actionable insights. We present an explainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by factoring PLM embeddings into two components: an interpretable subspace based on established biochemical features, and a residual subspace that preserves the model's predictive power. Using embeddings from ESM2, our adapter incorporates well-established properties, including secondary structure and hydropathy while maintaining high performance. We demonstrate the effectiveness of our approach across three protein-level classification tasks: prediction of extracellular vesicle association, identification of transmembrane helices, and prediction of aggregation propensity. PLM-X enables biological interpretation of model decisions without sacrificing accuracy, offering a generalisable solution for enhancing PLM interpretability across various downstream applications. This work addresses a critical need in computational biology by providing a bridge between powerful deep learning models and actionable biological insights.", "field": ["Biomolecules", "Artificial Intelligence", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzE1Ng"}, {"title": "Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs", "authors": ["Urszula Czerwinska"], "date": "April 2025", "doi": "arXiv:2504.07567", "url": "https://arxiv.org/abs/2504.07567", "pdf": "https://arxiv.org/pdf/2504.07567", "abstract": "We benchmark foundation models image embeddings for classification and retrieval in e-Commerce, evaluating their suitability for real-world applications. Our study spans embeddings from pre-trained convolutional and transformer models trained via supervised, self-supervised, and text-image contrastive learning. We assess full fine-tuning and transfer learning (top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars, food, and retail. Results show full fine-tuning consistently performs well, while text-image and self-supervised embeddings can match its performance with less training. While supervised embeddings remain stable across architectures, SSL and contrastive embeddings vary significantly, often benefiting from top-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning, reducing computational costs. We also explore cross-tuning, noting its impact depends on dataset characteristics. Our findings offer practical guidelines for embedding selection and fine-tuning strategies, balancing efficiency and performance.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence", "Computational Engineering, Finance, and Science", "Information Retrieval", "Machine Learning"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzU2Nw"}, {"title": "PRAD: Periapical Radiograph Analysis Dataset and Benchmark Model Development", "authors": ["Zhenhuan Zhou"], "date": "April 2025", "doi": "arXiv:2504.07760", "url": "https://arxiv.org/abs/2504.07760", "pdf": "https://arxiv.org/pdf/2504.07760", "abstract": "Deep learning (DL), a pivotal technology in artificial intelligence, has recently gained substantial traction in the domain of dental auxiliary diagnosis. However, its application has predominantly been confined to imaging modalities such as panoramic radiographs and Cone Beam Computed Tomography, with limited focus on auxiliary analysis specifically targeting Periapical Radiographs (PR). PR are the most extensively utilized imaging modality in endodontics and periodontics due to their capability to capture detailed local lesions at a low cost. Nevertheless, challenges such as resolution limitations and artifacts complicate the annotation and recognition of PR, leading to a scarcity of publicly available, large-scale, high-quality PR analysis datasets. This scarcity has somewhat impeded the advancement of DL applications in PR analysis. In this paper, we present PRAD-10K, a dataset for PR analysis. PRAD-10K comprises 10,000 clinical periapical radiograph images, with pixel-level annotations provided by professional dentists for nine distinct anatomical structures, lesions, and artificial restorations or medical devices, We also include classification labels for images with typical conditions or lesions. Furthermore, we introduce a DL network named PRNet to establish benchmarks for PR segmentation tasks. Experimental results demonstrate that PRNet surpasses previous state-of-the-art medical image segmentation models on the PRAD-10K dataset. The codes and dataset will be made publicly available.", "field": ["Image and Video Processing", "Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzc2MA"}, {"title": "PAYADOR: A Minimalist Approach to Grounding Language Models on Structured Data for Interactive Storytelling and Role-playing Games", "authors": ["Santiago G\u00f3ngora"], "date": "April 2025", "doi": "arXiv:2504.07304", "url": "https://arxiv.org/abs/2504.07304", "pdf": "https://arxiv.org/pdf/2504.07304", "abstract": "Every time an Interactive Storytelling (IS) system gets a player input, it is facing the world-update problem. Classical approaches to this problem consist in mapping that input to known preprogrammed actions, what can severely constrain the free will of the player. When the expected experience has a strong focus on improvisation, like in Role-playing Games (RPGs), this problem is critical. In this paper we present PAYADOR, a different approach that focuses on predicting the outcomes of the actions instead of representing the actions themselves. To implement this approach, we ground a Large Language Model to a minimal representation of the fictional world, obtaining promising results. We make this contribution open-source, so it can be adapted and used for other related research on unleashing the co-creativity power of RPGs.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzMwNA"}, {"title": "Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning Strategies", "authors": ["Julia Werner"], "date": "April 2025", "doi": "arXiv:2504.06039", "url": "https://arxiv.org/abs/2504.06039", "pdf": "https://arxiv.org/pdf/2504.06039", "abstract": "Capsule endoscopy is a method to capture images of the gastrointestinal tract and screen for diseases which might remain hidden if investigated with standard endoscopes. Due to the limited size of a video capsule, embedding AI models directly into the capsule demands careful consideration of the model size and thus complicates anomaly detection in this field. Furthermore, the scarcity of available data in this domain poses an ongoing challenge to achieving effective anomaly detection. Thus, this work introduces an ensemble strategy to address this challenge in anomaly detection tasks in video capsule endoscopies, requiring only a small number of individual neural networks during both the training and inference phases. Ensemble learning combines the predictions of multiple independently trained neural networks. This has shown to be highly effective in enhancing both the accuracy and robustness of machine learning models. However, this comes at the cost of higher memory usage and increased computational effort, which quickly becomes prohibitive in many real-world applications. Instead of applying the same training algorithm to each individual network, we propose using various loss functions, drawn from the anomaly detection field, to train each network. The methods are validated on the two largest publicly available datasets for video capsule endoscopy images, the Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on the Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our approach outperforms current baselines with significantly fewer parameters across all models, which is a crucial step towards incorporating artificial intelligence into capsule endoscopies.", "field": ["Computer Vision and Pattern Recognition"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjAzOQ"}, {"title": "Integrating Identity-Based Identification against Adaptive Adversaries in Federated Learning", "authors": ["Jakub Kacper Szelag"], "date": "April 2025", "doi": "arXiv:2504.03077", "url": "https://arxiv.org/abs/2504.03077", "pdf": "https://arxiv.org/pdf/2504.03077", "abstract": "Federated Learning (FL) has recently emerged as a promising paradigm for privacy-preserving, distributed machine learning. However, FL systems face significant security threats, particularly from adaptive adversaries capable of modifying their attack strategies to evade detection. One such threat is the presence of Reconnecting Malicious Clients (RMCs), which exploit FLs open connectivity by reconnecting to the system with modified attack strategies. To address this vulnerability, we propose integration of Identity-Based Identification (IBI) as a security measure within FL environments. By leveraging IBI, we enable FL systems to authenticate clients based on cryptographic identity schemes, effectively preventing previously disconnected malicious clients from re-entering the system. Our approach is implemented using the TNC-IBI (Tan-Ng-Chin) scheme over elliptic curves to ensure computational efficiency, particularly in resource-constrained environments like Internet of Things (IoT). Experimental results demonstrate that integrating IBI with secure aggregation algorithms, such as Krum and Trimmed Mean, significantly improves FL robustness by mitigating the impact of RMCs. We further discuss the broader implications of IBI in FL security, highlighting research directions for adaptive adversary detection, reputation-based mechanisms, and the applicability of identity-based cryptographic frameworks in decentralized FL architectures. Our findings advocate for a holistic approach to FL security, emphasizing the necessity of proactive defence strategies against evolving adaptive adversarial threats.", "field": ["Cryptography and Security", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzA3Nw"}, {"title": "DynMoLE: Boosting Mixture of LoRA Experts Fine-Tuning with a Hybrid Routing Mechanism", "authors": ["Dengchun Li"], "date": "April 2025", "doi": "arXiv:2504.00661", "url": "https://arxiv.org/abs/2504.00661", "pdf": "https://arxiv.org/pdf/2504.00661", "abstract": "Instruction-based fine-tuning of large language models (LLMs) has achieved remarkable success in various natural language processing (NLP) tasks. Parameter-efficient fine-tuning (PEFT) methods, such as Mixture of LoRA Experts (MoLE), combine the efficiency of Low-Rank Adaptation (LoRA) with the versatility of Mixture of Experts (MoE) models, demonstrating significant potential for handling multiple downstream tasks. However, the existing routing mechanisms for MoLE often involve a trade-off between computational efficiency and predictive accuracy, and they fail to fully address the diverse expert selection demands across different transformer layers. In this work, we propose DynMoLE, a hybrid routing strategy that dynamically adjusts expert selection based on the Tsallis entropy of the router's probability distribution. This approach mitigates router uncertainty, enhances stability, and promotes more equitable expert participation, leading to faster convergence and improved model performance. Additionally, we introduce an auxiliary loss based on Tsallis entropy to further guide the model toward convergence with reduced uncertainty, thereby improving training stability and performance. Our extensive experiments on commonsense reasoning benchmarks demonstrate that DynMoLE achieves substantial performance improvements, outperforming LoRA by 9.6% and surpassing the state-of-the-art MoLE method, MoLA, by 2.3%. We also conduct a comprehensive ablation study to evaluate the contributions of DynMoLE's key components.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDY2MQ"}, {"title": "Improving Counterfactual Truthfulness for Molecular Property Prediction through Uncertainty Quantification", "authors": ["Jonas Teufel"], "date": "April 2025", "doi": "arXiv:2504.02606", "url": "https://arxiv.org/abs/2504.02606", "pdf": "https://arxiv.org/pdf/2504.02606", "abstract": "Explainable AI (xAI) interventions aim to improve interpretability for complex black-box models, not only to improve user trust but also as a means to extract scientific insights from high-performing predictive systems. In molecular property prediction, counterfactual explanations offer a way to understand predictive behavior by highlighting which minimal perturbations in the input molecular structure cause the greatest deviation in the predicted property. However, such explanations only allow for meaningful scientific insights if they reflect the distribution of the true underlying property -- a feature we define as counterfactual truthfulness. To increase this truthfulness, we propose the integration of uncertainty estimation techniques to filter counterfactual candidates with high predicted uncertainty. Through computational experiments with synthetic and real-world datasets, we demonstrate that traditional uncertainty estimation methods, such as ensembles and mean-variance estimation, can already substantially reduce the average prediction error and increase counterfactual truthfulness, especially for out-of-distribution settings. Our results highlight the importance and potential impact of incorporating uncertainty estimation into explainability methods, especially considering the relatively high effectiveness of low-effort interventions like model ensembles.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjYwNg"}, {"title": "Steiner Traveling Salesman Problem with Quantum Annealing", "authors": ["Alessia Ciacco"], "date": "April 2025", "doi": "arXiv:2504.02388", "url": "https://arxiv.org/abs/2504.02388", "pdf": "https://arxiv.org/pdf/2504.02388", "abstract": "The Steiner Traveling Salesman Problem (STSP) is a variant of the classical Traveling Salesman Problem. The STSP involves incorporating steiner nodes, which are extra nodes not originally part of the required visit set but that can be added to the route to enhance the overall solution and minimize the total travel cost. Given the NP-hard nature of the STSP, we propose a quantum approach to address it. Specifically, we employ quantum annealing using D-Wave's hardware to explore its potential for solving this problem. To enhance computational feasibility, we develop a preprocessing method that effectively reduces the network size. Our experimental results demonstrate that this reduction technique significantly decreases the problem complexity, making the Quadratic Unconstrained Binary Optimization formulation, the standard input for quantum annealers, better suited for existing quantum hardware. Furthermore, the results highlight the potential of quantum annealing as a promising and innovative approach for solving the STSP.", "field": ["Quantum Physics", "Artificial Intelligence", "Emerging Technologies"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMjM4OA"}, {"title": "PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks", "authors": ["Marius Almanst\u00f6tter"], "date": "April 2025", "doi": "arXiv:2504.05248", "url": "https://arxiv.org/abs/2504.05248", "pdf": "https://arxiv.org/pdf/2504.05248", "abstract": "Parameter estimation for differential equations from measured data is an inverse problem prevalent across quantitative sciences. Physics-Informed Neural Networks (PINNs) have emerged as effective tools for solving such problems, especially with sparse measurements and incomplete system information. However, PINNs face convergence issues, stability problems, overfitting, and complex loss function design. Here we introduce PINNverse, a training paradigm that addresses these limitations by reformulating the learning process as a constrained differential optimization problem. This approach achieves a dynamic balance between data loss and differential equation residual loss during training while preventing overfitting. PINNverse combines the advantages of PINNs with the Modified Differential Method of Multipliers to enable convergence on any point on the Pareto front. We demonstrate robust and accurate parameter estimation from noisy data in four classical ODE and PDE models from physics and biology. Our method enables accurate parameter inference also when the forward problem is expensive to solve.", "field": ["Machine Learning", "Artificial Intelligence", "Computational Physics"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTI0OA"}, {"title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification", "authors": ["Bibek Paudel"], "date": "April 2025", "doi": "arXiv:2504.07069", "url": "https://arxiv.org/abs/2504.07069", "pdf": "https://arxiv.org/pdf/2504.07069", "abstract": "This paper introduces a comprehensive system for detecting hallucinations in large language model (LLM) outputs in enterprise settings. We present a novel taxonomy of LLM responses specific to hallucination in enterprise applications, categorizing them into context-based, common knowledge, enterprise-specific, and innocuous statements. Our hallucination detection model HDM-2 validates LLM responses with respect to both context and generally known facts (common knowledge). It provides both hallucination scores and word-level annotations, enabling precise identification of problematic content. To evaluate it on context-based and common-knowledge hallucinations, we introduce a new dataset HDMBench. Experimental results demonstrate that HDM-2 out-performs existing approaches across RagTruth, TruthfulQA, and HDMBench datasets. This work addresses the specific challenges of enterprise deployment, including computational efficiency, domain specialization, and fine-grained error identification. Our evaluation dataset, model weights, and inference code are publicly available.", "field": ["Computation and Language", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzA2OQ"}, {"title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation", "authors": ["Thomas Kerdreux"], "date": "April 2025", "doi": "arXiv:2504.06962", "url": "https://arxiv.org/abs/2504.06962", "pdf": "https://arxiv.org/pdf/2504.06962", "abstract": "Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to improve SSL pre-training by maximizing dataset diversity and balance. Our method iteratively refines the training set without requiring a pre-existing feature extractor, making it well-suited for domains where curated datasets are limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by ocean observations. We train models from scratch on the entire Sentinel-1 WV archive spanning 10 years. Across three downstream tasks, our results show that dynamic pruning improves both computational efficiency and representation quality, leading to stronger transferability.\n  We also release the weights of Nereus-SAR-1, the first model in the Nereus family, a series of foundation models for ocean observation and analysis using SAR imagery, at github.com/galeio-research/nereus-sar-models/.", "field": ["Computer Vision and Pattern Recognition", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNjk2Mg"}, {"title": "Drawing a Map of Elections", "authors": ["Stanis\u0142aw Szufa"], "date": "April 2025", "doi": "arXiv:2504.03809", "url": "https://arxiv.org/abs/2504.03809", "pdf": "https://arxiv.org/pdf/2504.03809", "abstract": "Our main contribution is the introduction of the map of elections framework. A map of elections consists of three main elements: (1) a dataset of elections (i.e., collections of ordinal votes over given sets of candidates), (2) a way of measuring similarities between these elections, and (3) a representation of the elections in the 2D Euclidean space as points, so that the more similar two elections are, the closer are their points. In our maps, we mostly focus on datasets of synthetic elections, but we also show an example of a map over real-life ones. To measure similarities, we would have preferred to use, e.g., the isomorphic swap distance, but this is infeasible due to its high computational complexity. Hence, we propose polynomial-time computable positionwise distance and use it instead. Regarding the representations in 2D Euclidean space, we mostly use the Kamada-Kawai algorithm, but we also show two alternatives. We develop the necessary theoretical results to form our maps and argue experimentally that they are accurate and credible. Further, we show how coloring the elections in a map according to various criteria helps in analyzing results of a number of experiments. In particular, we show colorings according to the scores of winning candidates or committees, running times of ILP-based winner determination algorithms, and approximation ratios achieved by particular algorithms.", "field": ["Multiagent Systems", "Artificial Intelligence", "Computer Science and Game Theory"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzgwOQ"}, {"title": "Deep Reinforcement Learning Algorithms for Option Hedging", "authors": ["Andrei Neagu"], "date": "April 2025", "doi": "arXiv:2504.05521", "url": "https://arxiv.org/abs/2504.05521", "pdf": "https://arxiv.org/pdf/2504.05521", "abstract": "Dynamic hedging is a financial strategy that consists in periodically transacting one or multiple financial assets to offset the risk associated with a correlated liability. Deep Reinforcement Learning (DRL) algorithms have been used to find optimal solutions to dynamic hedging problems by framing them as sequential decision-making problems. However, most previous work assesses the performance of only one or two DRL algorithms, making an objective comparison across algorithms difficult. In this paper, we compare the performance of eight DRL algorithms in the context of dynamic hedging; Monte Carlo Policy Gradient (MCPG), Proximal Policy Optimization (PPO), along with four variants of Deep Q-Learning (DQL) and two variants of Deep Deterministic Policy Gradient (DDPG). Two of these variants represent a novel application to the task of dynamic hedging. In our experiments, we use the Black-Scholes delta hedge as a baseline and simulate the dataset using a GJR-GARCH(1,1) model. Results show that MCPG, followed by PPO, obtain the best performance in terms of the root semi-quadratic penalty. Moreover, MCPG is the only algorithm to outperform the Black-Scholes delta hedge baseline with the allotted computational budget, possibly due to the sparsity of rewards in our environment.", "field": ["Computational Finance", "Artificial Intelligence", "Computational Engineering, Finance, and Science"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNTUyMQ"}, {"title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations", "authors": ["Maik Larooij"], "date": "April 2025", "doi": "arXiv:2504.03274", "url": "https://arxiv.org/abs/2504.03274", "pdf": "https://arxiv.org/pdf/2504.03274", "abstract": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as the integration of Large Language Models (LLMs) has led to the emergence of ``generative ABMs'' as a novel approach to simulating social systems. While ABMs offer means to bridge micro-level interactions with macro-level patterns, they have long faced criticisms from social scientists, pointing to e.g., lack of realism, computational complexity, and challenges of calibrating and validating against empirical data. This paper reviews the generative ABM literature to assess how this new approach adequately addresses these long-standing criticisms. Our findings show that studies show limited awareness of historical debates. Validation remains poorly addressed, with many studies relying solely on subjective assessments of model `believability', and even the most rigorous validation failing to adequately evidence operational validity. We argue that there are reasons to believe that LLMs will exacerbate rather than resolve the long-standing challenges of ABMs. The black-box nature of LLMs moreover limit their usefulness for disentangling complex emergent causal mechanisms. While generative ABMs are still in a stage of early experimentation, these findings question of whether and how the field can transition to the type of rigorous modeling needed to contribute to social scientific theory.", "field": ["Multiagent Systems", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMzI3NA"}, {"title": "Cellular Development Follows the Path of Minimum Action", "authors": ["Rohola Zandie"], "date": "April 2025", "doi": "arXiv:2504.08096", "url": "https://arxiv.org/abs/2504.08096", "pdf": "https://arxiv.org/pdf/2504.08096", "abstract": "Cellular development follows a stochastic yet rule-governed trajectory, though the underlying principles remain elusive. Here, we propose that cellular development follows paths of least action, aligning with foundational physical laws that govern dynamic systems across nature. We introduce a computational framework that takes advantage of the deep connection between the principle of least action and maximum entropy to model developmental processes using Transformers architecture. This approach enables precise quantification of entropy production, information flow curvature, and local irreversibility for developmental asymmetry in single-cell RNA sequence data. Within this unified framework, we provide interpretable metrics: entropy to capture exploration-exploitation trade-offs, curvature to assess plasticity-elasticity dynamics, and entropy production to characterize dedifferentiation and transdifferentiation. We validate our method across both single-cell and embryonic development datasets, demonstrating its ability to reveal hidden thermodynamic and informational constraints shaping cellular fate decisions.", "field": ["Biological Physics", "Artificial Intelligence", "Computational Physics"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wODA5Ng"}, {"title": "Explainability and Continual Learning meet Federated Learning at the Network Edge", "authors": ["Thomas Tsouparopoulos"], "date": "April 2025", "doi": "arXiv:2504.08536", "url": "https://arxiv.org/abs/2504.08536", "pdf": "https://arxiv.org/pdf/2504.08536", "abstract": "As edge devices become more capable and pervasive in wireless networks, there is growing interest in leveraging their collective compute power for distributed learning. However, optimizing learning at the network edge entails unique challenges, particularly when moving beyond conventional settings and objectives. While Federated Learning (FL) has emerged as a key paradigm for distributed model training, critical challenges persist. First, existing approaches often overlook the trade-off between predictive accuracy and interpretability. Second, they struggle to integrate inherently explainable models such as decision trees because their non-differentiable structure makes them not amenable to backpropagation-based training algorithms. Lastly, they lack meaningful mechanisms for continual Machine Learning (ML) model adaptation through Continual Learning (CL) in resource-limited environments. In this paper, we pave the way for a set of novel optimization problems that emerge in distributed learning at the network edge with wirelessly interconnected edge devices, and we identify key challenges and future directions. Specifically, we discuss how Multi-objective optimization (MOO) can be used to address the trade-off between predictive accuracy and explainability when using complex predictive models. Next, we discuss the implications of integrating inherently explainable tree-based models into distributed learning settings. Finally, we investigate how CL strategies can be effectively combined with FL to support adaptive, lifelong learning when limited-size buffers are used to store past data for retraining. Our approach offers a cohesive set of tools for designing privacy-preserving, adaptive, and trustworthy ML solutions tailored to the demands of edge computing and intelligent services.", "field": ["Machine Learning", "Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wODUzNg"}, {"title": "Dual Engines of Thoughts: A Depth-Breadth Integration Framework for Open-Ended Analysis", "authors": ["Fei-Hsuan Yu"], "date": "April 2025", "doi": "arXiv:2504.07872", "url": "https://arxiv.org/abs/2504.07872", "pdf": "https://arxiv.org/pdf/2504.07872", "abstract": "We propose the Dual Engines of Thoughts (DEoT), an analytical framework for comprehensive open-ended reasoning. While traditional reasoning frameworks primarily focus on finding \"the best answer\" or \"the correct answer\" for single-answer problems, DEoT is specifically designed for \"open-ended questions,\" enabling both broader and deeper analytical exploration. The framework centers on three key components: a Base Prompter for refining user queries, a Solver Agent that orchestrates task decomposition, execution, and validation, and a Dual-Engine System consisting of a Breadth Engine (to explore diverse impact factors) and a Depth Engine (to perform deep investigations). This integrated design allows DEoT to balance wide-ranging coverage with in-depth analysis, and it is highly customizable, enabling users to adjust analytical parameters and tool configurations based on specific requirements. Experimental results show that DEoT excels in addressing complex, multi-faceted questions, achieving a total win rate of 77-86% compared to existing reasoning models, thus highlighting its effectiveness in real-world applications.", "field": ["Artificial Intelligence", "Computational Engineering, Finance, and Science", "Computation and Language", "Multiagent Systems"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wNzg3Mg"}, {"title": "Exploration and Adaptation in Non-Stationary Tasks with Diffusion Policies", "authors": ["Gunbir Singh Baveja"], "date": "April 2025", "doi": "arXiv:2504.00280", "url": "https://arxiv.org/abs/2504.00280", "pdf": "https://arxiv.org/pdf/2504.00280", "abstract": "This paper investigates the application of Diffusion Policy in non-stationary, vision-based RL settings, specifically targeting environments where task dynamics and objectives evolve over time. Our work is grounded in practical challenges encountered in dynamic real-world scenarios such as robotics assembly lines and autonomous navigation, where agents must adapt control strategies from high-dimensional visual inputs. We apply Diffusion Policy -- which leverages iterative stochastic denoising to refine latent action representations-to benchmark environments including Procgen and PointMaze. Our experiments demonstrate that, despite increased computational demands, Diffusion Policy consistently outperforms standard RL methods such as PPO and DQN, achieving higher mean and maximum rewards with reduced variability. These findings underscore the approach's capability to generate coherent, contextually relevant action sequences in continuously shifting conditions, while also highlighting areas for further improvement in handling extreme non-stationarity.", "field": ["Artificial Intelligence"], "link": "http://127.0.0.1:5425/paper/YXJYaXY6MjUwNC4wMDI4MA"}]
    
            data.forEach((item) => {

                const itemElement = document.createElement('div');
                itemElement.classList.add('dyn-scrap-item');


                abst = item.abstract.substring(0,400);
    
                itemElement.innerHTML = `
                    <img src="../assets/img/dynax.svg">
                    <h3>${item.title}</h3>
                    <div class="info-line"><strong> Authors: </strong> ${item.authors} </div>
                    <div class="info-line"><strong> Year: </strong> ${item.date} </div>
                    <div class="info-line"><strong> DOI: </strong> ${item.doi} </div>
                    <div class="info-line"><strong> Subject: </strong> ${item.field} </div>
                    <div class="info-line"><strong> Abstract: </strong> ${abst}... </div>

                    <div class="metadata">
                        <a href="${item.link}" target=""> Explore → </a>
                    </div>
                `;
    
                scrapItemsContainer.appendChild(itemElement);
            });
        } catch (error) {

            console.error('Error fetching research papers:', error);

            alert('Failed to Fetch Research Papers. Please Try Again Later.');

        } finally {

            hideLoading(),1000
        }
    }
    
        
    searchButton.addEventListener("click", function () {
        const query = searchInput.value.trim();

        if (query) {
            fetchResearchPapers(query);
        } else {
            alert("Please enter a search query!");
        }
    });
    
    searchInput.addEventListener("keypress", function (e) {
        if (e.key === "Enter") {
            const query = searchInput.value.trim();

            if (query) {
                fetchResearchPapers(query);
            } else {
                alert("Please enter a search query!");
            }
        }
    });
});

